[
["index.html", "Inferential Statistics Course Activity Preface", " Inferential Statistics Course Activity Akshay Kotha 2018-12-29 Preface Check out the list of weeks and the project to the left. "],
["week1.html", "1 Week 1 1.1 Getting Started 1.2 The unknown sampling distribution 1.3 Interlude: Sampling distributions 1.4 Sample size and the sampling distribution", " 1 Week 1 Week 1 covers calculating sampling distributions. Below is the activity. Complete all Exercises, and submit answers to Questions on the Coursera platform. 1.1 Getting Started 1.1.1 Load packages In this lab we will explore the data using the dplyr package and visualize it using the ggplot2 package for data visualization. The data can be found in the companion package for this course, statsr. Let’s load the packages. library(statsr) library(dplyr) library(shiny) library(ggplot2) 1.1.2 The data We consider real estate data from the city of Ames, Iowa. The details of every real estate transaction in Ames is recorded by the City Assessor’s office. Our particular focus for this lab will be all residential home sales in Ames between 2006 and 2010. This collection represents our population of interest. In this lab we would like to learn about these home sales by taking smaller samples from the full population. Let’s load the data. data(ames) We see that there are quite a few variables in the data set, enough to do a very in-depth analysis. For this lab, we’ll restrict our attention to just two of the variables: the above ground living area of the house in square feet (area) and the sale price (price). We can explore the distribution of areas of homes in the population of home sales visually and with summary statistics. Let’s first create a visualization, a histogram: ggplot(data = ames, aes(x = area)) + geom_histogram(binwidth = 250) Let’s also obtain some summary statistics. Note that we can do this using the summarise function. We can calculate as many statistics as we want using this function, and just string along the results. Some of the functions below should be self explanatory (like mean, median, sd, IQR, min, and max). A new function here is the quantile function which we can use to calculate values corresponding to specific percentile cutoffs in the distribution. For example quantile(x, 0.25) will yield the cutoff value for the 25th percentile (Q1) in the distribution of x. Finding these values are useful for describing the distribution, as we can use them for descriptions like “the middle 50% of the homes have areas between such and such square feet”. ames %&gt;% summarise(mu = mean(area), pop_med = median(area), sigma = sd(area), pop_iqr = IQR(area), pop_min = min(area), pop_max = max(area), pop_q1 = quantile(area, 0.25), # first quartile, 25th percentile pop_q3 = quantile(area, 0.75)) # third quartile, 75th percentile ## # A tibble: 1 x 8 ## mu pop_med sigma pop_iqr pop_min pop_max pop_q1 pop_q3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1500. 1442 506. 617. 334 5642 1126 1743. Which of the following is false? The distribution of areas of houses in Ames is unimodal and right-skewed. 50% of houses in Ames are smaller than 1,499.69 square feet. The middle 50% of the houses range between approximately 1,126 square feet and 1,742.7 square feet. The IQR is approximately 616.7 square feet. The smallest house is 334 square feet and the largest is 5,642 square feet. 1.2 The unknown sampling distribution In this lab we have access to the entire population, but this is rarely the case in real life. Gathering information on an entire population is often extremely costly or impossible. Because of this, we often take a sample of the population and use that to understand the properties of the population. If we were interested in estimating the mean living area in Ames based on a sample, we can use the following command to survey the population. samp1 &lt;- ames %&gt;% sample_n(size = 50) This command collects a simple random sample of size 50 from the ames dataset, which is assigned to samp1. This is like going into the City Assessor’s database and pulling up the files on 50 random home sales. Working with these 50 files would be considerably simpler than working with all 2930 home sales. Exercise: Describe the distribution of this sample? How does it compare to the distribution of the population? Hint: sample_n function takes a random sample of observations (i.e. rows) from the dataset, you can still refer to the variables in the dataset with the same names. Code you used in the previous exercise will also be helpful for visualizing and summarizing the sample, however be careful to not label values mu and sigma anymore since these are sample statistics, not population parameters. You can customize the labels of any of the statistics to indicate that these come from the sample. # type your code for the Exercise here, and Run Document ggplot(samp1, aes(x = area)) + geom_histogram(binwidth = 200) # The distribution is symmetric unlike the population distribution If we’re interested in estimating the average living area in homes in Ames using the sample, our best single guess is the sample mean. samp1 %&gt;% summarise(x_bar = mean(area)) ## # A tibble: 1 x 1 ## x_bar ## &lt;dbl&gt; ## 1 1271. Depending on which 50 homes you selected, your estimate could be a bit above or a bit below the true population mean of 1,499.69 square feet. In general, though, the sample mean turns out to be a pretty good estimate of the average living area, and we were able to get it by sampling less than 3% of the population. Suppose we took two more samples, one of size 100 and one of size 1000. Which would you think would provide a more accurate estimate of the population mean? Sample size of 50. Sample size of 100. Sample size of 1000. Let’s take one more sample of size 50, and view the mean area in this sample: ames %&gt;% sample_n(size = 1000) %&gt;% summarise(x_bar = mean(area)) ## # A tibble: 1 x 1 ## x_bar ## &lt;dbl&gt; ## 1 1506. Not surprisingly, every time we take another random sample, we get a different sample mean. It’s useful to get a sense of just how much variability we should expect when estimating the population mean this way. The distribution of sample means, called the sampling distribution, can help us understand this variability. In this lab, because we have access to the population, we can build up the sampling distribution for the sample mean by repeating the above steps many times. Here we will generate 15,000 samples and compute the sample mean of each. Note that we are sampling with replacement, replace = TRUE since sampling distributions are constructed with sampling with replacement. sample_means50 &lt;- ames %&gt;% rep_sample_n(size = 50, reps = 15000, replace = TRUE) %&gt;% summarise(x_bar = mean(area)) ggplot(data = sample_means50, aes(x = x_bar)) + geom_histogram(binwidth = 20) Here we use R to take 15,000 samples of size 50 from the population, calculate the mean of each sample, and store each result in a vector called sample_means50. Next, we review how this set of code works. Exercise: How many elements are there in sample_means50? Describe the sampling distribution, and be sure to specifically note its center. Make sure to include a plot of the distribution in your answer. # type your code for the Exercise here, and Run Document #No. of elements in sample_means50 dim(sample_means50) ## [1] 15000 2 # Center of sampling distribution sample_means50 %&gt;% summarise(means = mean(x_bar)) ## # A tibble: 1 x 1 ## means ## &lt;dbl&gt; ## 1 1500. #Given that 15000 samples are considered, the distribution resembles a normal distribution and it looks symmetric. Sample means vary around the population mean equally. 1.3 Interlude: Sampling distributions The idea behind the rep_sample_n function is repetition. Earlier we took a single sample of size n (50) from the population of all houses in Ames. With this new function we are able to repeat this sampling procedure rep times in order to build a distribution of a series of sample statistics, which is called the sampling distribution. Note that in practice one rarely gets to build sampling distributions, because we rarely have access to data from the entire population. Without the rep_sample_n function, this would be painful. We would have to manually run the following code 15,000 times ames %&gt;% sample_n(size = 50) %&gt;% summarise(x_bar = mean(area)) as well as store the resulting sample means each time in a separate vector. Note that for each of the 15,000 times we computed a mean, we did so from a different sample! Exercise: To make sure you understand how sampling distributions are built, and exactly what the sample_n and do function do, try modifying the code to create a sampling distribution of 25 sample means from samples of size 10, and put them in a data frame named sample_means_small. Print the output. How many observations are there in this object called sample_means_small? What does each observation represent? # type your code for the Exercise here, and Run Document sample_means_small &lt;- ames %&gt;% rep_sample_n(size = 10, reps = 25, replace = TRUE) %&gt;% summarise(x_bar_practice = mean(area)) How many elements are there in this object called sample_means_small? 0 3 25 100 5,000 # type your code for Question 3 here, and Run Document dim(sample_means_small) ## [1] 25 2 sample_means_small ## # A tibble: 25 x 2 ## replicate x_bar_practice ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1641. ## 2 2 1465. ## 3 3 1680 ## 4 4 1395. ## 5 5 1488. ## 6 6 1591. ## 7 7 1461. ## 8 8 1406. ## 9 9 1366 ## 10 10 1257. ## # ... with 15 more rows Which of the following is true about the elements in the sampling distributions you created? Each element represents a mean square footage from a simple random sample of 10 houses. Each element represents the square footage of a house. Each element represents the true population mean of square footage of houses. 1.4 Sample size and the sampling distribution Mechanics aside, let’s return to the reason we used the rep_sample_n function: to compute a sampling distribution, specifically, this one. ggplot(data = sample_means50, aes(x = x_bar)) + geom_histogram(binwidth = 20) The sampling distribution that we computed tells us much about estimating the average living area in homes in Ames. Because the sample mean is an unbiased estimator, the sampling distribution is centered at the true average living area of the population, and the spread of the distribution indicates how much variability is induced by sampling only 50 home sales. In the remainder of this section we will work on getting a sense of the effect that sample size has on our sampling distribution. Exercise: Use the app below to create sampling distributions of means of areas from samples of size 10, 50, and 100. Use 5,000 simulations. What does each observation in the sampling distribution represent? How does the mean, standard error, and shape of the sampling distribution change as the sample size increases? How (if at all) do these values change if you increase the number of simulations? Shiny applications not supported in static R Markdown documents It makes intuitive sense that as the sample size increases, the center of the sampling distribution becomes a more reliable estimate for the true population mean. Also as the sample size increases, the variability of the sampling distribution ________. decreases increases stays the same Exercise: Take a random sample of size 50 from price. Using this sample, what is your best point estimate of the population mean? # type your code for this Exercise here, and Run Document samp1 %&gt;% summarise(mu_price = mean(price)) ## # A tibble: 1 x 1 ## mu_price ## &lt;dbl&gt; ## 1 161474. Exercise: Since you have access to the population, simulate the sampling distribution for \\(\\bar{x}_{price}\\) by taking 5000 samples from the population of size 50 and computing 5000 sample means. Store these means in a vector called sample_means50. Plot the data, then describe the shape of this sampling distribution. Based on this sampling distribution, what would you guess the mean home price of the population to be? # type your code for this Exercise here, and Run Document sample_means50 &lt;- ames %&gt;% rep_sample_n(size = 50, reps = 5000, replace = TRUE) %&gt;% summarise(x_bar_price = mean(price)) ggplot(sample_means50,aes(x = x_bar_price)) + geom_histogram(binwidth = 1000) Exercise: Change your sample size from 50 to 150, then compute the sampling distribution using the same method as above, and store these means in a new vector called sample_means150. Describe the shape of this sampling distribution, and compare it to the sampling distribution for a sample size of 50. Based on this sampling distribution, what would you guess to be the mean sale price of homes in Ames? # type your code for this Exercise here, and Run Document sample_means150 &lt;- ames %&gt;% rep_sample_n(size = 150, reps = 5000, replace = TRUE) %&gt;% summarise(x_bar_price = mean(price)) ggplot(sample_means150,aes(x = x_bar_price)) + geom_histogram(binwidth = 1000) So far, we have only focused on estimating the mean living area in homes in Ames. Now you’ll try to estimate the mean home price. Note that while you might be able to answer some of these questions using the app you are expected to write the required code and produce the necessary plots and summary statistics. You are welcomed to use the app for exploration. Exercise: Take a sample of size 15 from the population and calculate the mean price of the homes in this sample. Using this sample, what is your best point estimate of the population mean of prices of homes? # type your code for this Exercise here, and Run Document samp1price &lt;- ames %&gt;% sample_n(size = 15) samp1price %&gt;% summarise(mean(price)) ## # A tibble: 1 x 1 ## `mean(price)` ## &lt;dbl&gt; ## 1 196388. Exercise: Since you have access to the population, simulate the sampling distribution for \\(\\bar{x}_{price}\\) by taking 2000 samples from the population of size 15 and computing 2000 sample means. Store these means in a vector called sample_means15. Plot the data, then describe the shape of this sampling distribution. Based on this sampling distribution, what would you guess the mean home price of the population to be? Finally, calculate and report the population mean. # type your code for this Exercise here, and Run Document sample_means15 &lt;- ames %&gt;% rep_sample_n(size = 15, reps = 2000, replace = TRUE) %&gt;% summarise(x_bar_price15 = mean(price)) ggplot(sample_means15,aes(x = x_bar_price15)) + geom_histogram(binwidth = 1000) ames %&gt;% summarise(mu_price = mean(price)) ## # A tibble: 1 x 1 ## mu_price ## &lt;dbl&gt; ## 1 180796. Exercise: Change your sample size from 15 to 150, then compute the sampling distribution using the same method as above, and store these means in a new vector called sample_means150. Describe the shape of this sampling distribution, and compare it to the sampling distribution for a sample size of 15. Based on this sampling distribution, what would you guess to be the mean sale price of homes in Ames? # type your code for this Exercise here, and Run Document sample_means150 &lt;- ames %&gt;% rep_sample_n(size = 150, reps = 2000, replace = TRUE) %&gt;% summarise(x_bar_price150 = mean(price)) ggplot(sample_means150,aes(x = x_bar_price150)) + geom_histogram(binwidth = 1000) Which of the following is false? The variability of the sampling distribution with the smaller sample size (sample_means50) is smaller than the variability of the sampling distribution with the larger sample size (sample_means150). The means for the two sampling distribtuions are roughly similar. Both sampling distributions are symmetric. # type your code for Question 6 here, and Run Document # Comparing variability between two sampling distributions (n=50), (n =150) sample_means50 %&gt;% summarise( max(x_bar_price)-min(x_bar_price)) ## # A tibble: 1 x 1 ## `max(x_bar_price) - min(x_bar_price)` ## &lt;dbl&gt; ## 1 77994. sample_means150 %&gt;% summarise(max(x_bar_price150) - min(x_bar_price150)) ## # A tibble: 1 x 1 ## `max(x_bar_price150) - min(x_bar_price150)` ## &lt;dbl&gt; ## 1 46172. This is a derivative of an OpenIntro lab, and is released under a Attribution-NonCommercial-ShareAlike 3.0 United States license. "],
["week2.html", "2 Week 2 2.1 Getting Started 2.2 Confidence intervals 2.3 Confidence levels 2.4 How set.seed() works - testing?", " 2 Week 2 Week 2 covers calculating confidence intervals. Below is the activity. Complete all Exercises, and submit answers to Questions on the Coursera platform. If you have access to data on an entire population, say the size of every house in Ames, Iowa, it’s straight forward to answer questions like, “How big is the typical house in Ames?” and “How much variation is there in sizes of houses?”. If you have access to only a sample of the population, as is often the case, the task becomes more complicated. What is your best guess for the typical size if you only know the sizes of several dozen houses? This sort of situation requires that you use your sample to make inference on what your population looks like. Setting a seed: We will take some random samples and calculate confidence based on these samples in this lab, which means you should set a seed on top of your lab. If this concept is new to you, review the previous lab and ask your TA. Setting a seed will cause R to sample the same sample each time you knit your document. This will make sure your results don’t change each time you knit, and it will also ensure reproducibility of your work (by setting the same seed it will be possible to reproduce your results). You can set a seed like this: set.seed(19122018) # my seed The number above is completely arbitraty. If you need inspiration, you can use your ID, birthday, or just a random string of numbers. The important thing is that you use each seed only once. You only need to do this once in your R Markdown document, but make sure it comes before sampling. 2.1 Getting Started 2.1.1 Load packages In this lab we will explore the data using the dplyr package and visualize it using the ggplot2 package for data visualization. The data can be found in the companion package for this course, statsr. Let’s load the packages. library(statsr) library(dplyr) library(ggplot2) 2.1.2 The data We consider real estate data from the city of Ames, Iowa. This is the same dataset used in the previous lab. The details of every real estate transaction in Ames is recorded by the City Assessor’s office. Our particular focus for this lab will be all residential home sales in Ames between 2006 and 2010. This collection represents our population of interest. In this lab we would like to learn about these home sales by taking smaller samples from the full population. Let’s load the data. data(ames) In this lab we’ll start with a simple random sample of size 60 from the population. Specifically, this is a simple random sample of size 60. Note that the data set has information on many housing variables, but for the first portion of the lab we’ll focus on the size of the house, represented by the variable area. n &lt;- 60 samp &lt;- sample_n(ames, n) Exercise: Describe the distribution of homes in your sample. What would you say is the “typical” size within your sample? Also state precisely what you interpreted “typical” to mean. Answer: Typical size between 1000 and 1500 as it seems to be a bi-modal distribution.‘Typical’ meaning size which is more frequent. # type your code for the Exercise here, and Knit ggplot(samp, aes(x = area)) + geom_histogram(binwidth = 320) True or False: My distribution should be similar to others’ distributions who also collect random samples from this population, but it is likely not exactly the same since it’s a random sample. True. False. 2.2 Confidence intervals Return for a moment to the question that first motivated this lab: based on this sample, what can we infer about the population? Based only on this single sample, the best estimate of the average living area of houses sold in Ames would be the sample mean, usually denoted as \\(\\bar{x}\\) (here we’re calling it x_bar). That serves as a good point estimate but it would be useful to also communicate how uncertain we are of that estimate. This uncertainty can be quantified using a confidence interval. A confidence interval for a population mean is of the following form \\[ \\bar{x} \\pm z^\\star \\frac{s}{\\sqrt{n}} \\] You should by now be comfortable with calculating the mean and standard deviation of a sample in R. And we know that the sample size is 60. So the only remaining building block is finding the appropriate critical value for a given confidence level. We can use the qnorm function for this task, which will give the critical value associated with a given percentile under the normal distribution. Remember that confidence levels and percentiles are not equivalent. For example, a 95% confidence level refers to the middle 95% of the distribution, and the critical value associated with this area will correspond to the 97.5th percentile. We can find the critical value for a 95% confidence interal using z_star_95 &lt;- qnorm(0.975) z_star_95 ## [1] 1.959964 which is roughly equal to the value critical value 1.96 that you’re likely familiar with by now. Let’s finally calculate the confidence interval: samp %&gt;% summarise(lower = mean(area) - z_star_95 * (sd(area) / sqrt(n)), upper = mean(area) + z_star_95 * (sd(area) / sqrt(n))) ## # A tibble: 1 x 2 ## lower upper ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1403. 1631. To recap: even though we don’t know what the full population looks like, we’re 95% confident that the true average size of houses in Ames lies between the values lower and upper. There are a few conditions that must be met for this interval to be valid. For the confidence interval to be valid, the sample mean must be normally distributed and have standard error \\(s / \\sqrt{n}\\). Which of the following is not a condition needed for this to be true? The sample is random. The sample size, 60, is less than 10% of all houses. The sample distribution must be nearly normal. 2.3 Confidence levels What does “95% confidence” mean? 95% of the time the true average area of houses in Ames, Iowa, will be in this interval. 95% of random samples of size 60 will yield confidence intervals that contain the true average area of houses in Ames, Iowa. 95% of the houses in Ames have an area in this interval. 95% confident that the sample mean is in this interval. In this case we have the rare luxury of knowing the true population mean since we have data on the entire population. Let’s calculate this value so that we can determine if our confidence intervals actually capture it. We’ll store it in a data frame called params (short for population parameters), and name it mu. params &lt;- ames %&gt;% summarise(mu = mean(area)) Exercise: Does your confidence interval capture the true average size of houses in Ames? # type your code for the Exercise here, and Knit # Yes, the true mean is contained within the confidence interval. What proportion of 95% confidence intervals would you expect to capture the true population mean? 1% 5% 95% 99% Using R, we’re going to collect many samples to learn more about how sample means and confidence intervals vary from one sample to another. Here is the rough outline: Obtain a random sample. Calculate the sample’s mean and standard deviation, and use these to calculate and store the lower and upper bounds of the confidence intervals. Repeat these steps 50 times. We can accomplish this using the rep_sample_n function. The following lines of code takes 50 random samples of size n from population (and remember we defined \\(n = 60\\) earlier), and computes the upper and lower bounds of the confidence intervals based on these samples. set.seed(19122018); ci &lt;- ames %&gt;% rep_sample_n(size = n, reps = 50, replace = TRUE) %&gt;% summarise(lower = mean(area) - z_star_95 * (sd(area) / sqrt(n)), upper = mean(area) + z_star_95 * (sd(area) / sqrt(n)), samplemeans.95 = mean(area)) Let’s view the first five intervals: #set.seed(19122018); ci %&gt;% slice(1:5) ## # A tibble: 5 x 4 ## replicate lower upper samplemeans.95 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1382. 1608. 1495. ## 2 2 1341. 1605. 1473. ## 3 3 1221. 1439. 1330. ## 4 4 1417. 1682. 1549. ## 5 5 1300. 1574. 1437. Next we’ll create a plot similar to Figure 4.8 on page 175 of OpenIntro Statistics, 3rd Edition. First step will be to create a new variable in the ci data frame that indicates whether the interval does or does not capture the true population mean. Note that capturing this value would mean the lower bound of the confidence interval is below the value and upper bound of the confidence interval is above the value. Remember that we create new variables using the mutate function. #set.seed(19122018) ci &lt;- ci %&gt;% mutate(capture_mu = ifelse(lower &lt; params$mu &amp; upper &gt; params$mu, &quot;yes&quot;, &quot;no&quot;)) The ifelse function is new. It takes three arguments: first is a logical statement, second is the value we want if the logical statement yields a true result, and the third is the value we want if the logical statement yields a false result. We now have all the information we need to create the plot, but we need to re-organize our data a bit for easy plotting. Specifically, we need to organize the data in a new data frame where each row represents one bound, as opposed to one interval. So this lower upper capture_mu 1 1350.540 1544.360 yes 2 1333.441 1584.425 yes 3 1412.133 1663.801 yes ... should instead look like ci_id ci_bounds capture_mu 1 1 1350.540 yes 2 2 1333.441 yes 3 3 1412.133 yes 4 1 1544.360 yes 5 2 1584.425 yes 6 3 1663.801 yes ... We can accomplish this using the following: set.seed(19122018) ci_data &lt;- data.frame(ci_id = c(1:50, 1:50), ci_bounds = c(ci$lower, ci$upper), capture_mu = c(ci$capture_mu, ci$capture_mu)) And finally we can create the plot using the following: #set.seed(19122018) ggplot(data = ci_data, aes(x = ci_bounds, y = ci_id, group = ci_id, color = capture_mu)) + geom_point(size = 2) + # add points at the ends, size = 2 geom_line() + # connect with lines geom_vline(xintercept = params$mu, color = &quot;darkgray&quot;) # draw vertical line 2.4 How set.seed() works - testing? set.seed(111112) ci.95 &lt;- ames %&gt;% rep_sample_n(size = n, reps = 50, replace = TRUE) %&gt;% summarise(samplemeans.95 = mean(area)) head(ci.95) ## # A tibble: 6 x 2 ## replicate samplemeans.95 ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1526. ## 2 2 1474. ## 3 3 1492. ## 4 4 1473. ## 5 5 1446. ## 6 6 1483. set.seed(111112) ci.99 &lt;- ames %&gt;% rep_sample_n(size = n, reps = 50, replace = TRUE) %&gt;% summarise(samplemeans.99 = mean(area)) head(ci.99) ## # A tibble: 6 x 2 ## replicate samplemeans.99 ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1526. ## 2 2 1474. ## 3 3 1492. ## 4 4 1473. ## 5 5 1446. ## 6 6 1483. Exercise: What proportion of your confidence intervals include the true population mean? Is this proportion exactly equal to the confidence level? If not, explain why. Answer: It is almost all of the CIs = 98% include the true population mean except 1 sample. The proportion is not exactly equal to the confidence level because the confidence interval only provides a plausible range within which the mean is captured. BY definition, we are 95% confident, but results might vary based on how the samples are randonly selected. This is an overestimation of the 95% CI sampling. What is the appropriate critical value for a 99% confidence level? 0.01 0.99 1.96 2.33 2.58 # type your code for the Question 5 here, and Knit set.seed(19122018) z_star_99 &lt;- qnorm(0.995) z_star_99 ## [1] 2.575829 Exercise: Calculate 50 confidence intervals at the 99% confidence level. You do not need to obtain new samples, simply calculate new intervals based on the 95% confidence interval endpoints you had already collected. Plot all intervals and calculate the proportion of intervals that include the true population mean. # type your code for the Exercise here, and Knit set.seed(19122018) #samples for 99% confidence interval ci_99 &lt;- ames %&gt;% rep_sample_n(size = n ,reps = 50, replace = TRUE) %&gt;% summarise(lower = mean(area) - z_star_99 * (sd(area) / sqrt(n)), upper = mean(area) + z_star_99 * (sd(area) / sqrt(n)), samplemeans.99 = mean(area)) #assignment of capture ci_99 &lt;- ci_99 %&gt;% mutate(capture_mu_99 = ifelse(lower &lt; params$mu &amp; upper &gt; params$mu, &quot;yes&quot;, &quot;no&quot;)) #datagrame for plot ci_data_99 &lt;- data.frame(ci_id_99 = c(1:50, 1:50), ci_bounds_99 = c(ci_99$lower, ci_99$upper), capture_mu_99 = c(ci_99$capture_mu_99, ci_99$capture_mu_99)) #plot ggplot(data = ci_data_99, aes(x = ci_bounds_99, y = ci_id_99, group = ci_id_99, color = capture_mu_99)) + geom_point(size = 2) + # add points at the ends, size = 2 geom_line() + # connect with lines geom_vline(xintercept = params$mu, color = &quot;darkblue&quot;) # draw vertical line ci_99 %&gt;% slice(1:5) ## # A tibble: 5 x 5 ## replicate lower upper samplemeans.99 capture_mu_99 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 1347. 1644. 1495. yes ## 2 2 1300. 1647. 1473. yes ## 3 3 1187. 1473. 1330. no ## 4 4 1375. 1723. 1549. yes ## 5 5 1257. 1617. 1437. yes ``` We would expect 99% of the intervals to contain the true population mean. True False This is a product of OpenIntro that is released under a Creative Commons Attribution-ShareAlike 3.0 Unported. This lab was written for OpenIntro by Andrew Bray and Mine Çetinkaya-Rundel. "],
["week3.html", "3 Week 3 3.1 Getting Started 3.2 Exploratory data analysis 3.3 Inference", " 3 Week 3 Week 3 is about making inferences about numerical data. Complete all Exercises, and submit answers to Questions on the Coursera platform. 3.1 Getting Started 3.1.1 Load packages In this lab we will explore the data using the dplyr package and visualize it using the ggplot2 package for data visualization. The data can be found in the companion package for this course, statsr. Let’s load the packages. library(statsr) library(dplyr) library(ggplot2) 3.1.2 The data In 2004, the state of North Carolina released a large data set containing information on births recorded in this state. This data set is useful to researchers studying the relation between habits and practices of expectant mothers and the birth of their children. We will work with a random sample of observations from this data set. Load the nc data set into our workspace. data(nc) We have observations on 13 different variables, some categorical and some numerical. The meaning of each variable is as follows. variable description fage father’s age in years. mage mother’s age in years. mature maturity status of mother. weeks length of pregnancy in weeks. premie whether the birth was classified as premature (premie) or full-term. visits number of hospital visits during pregnancy. marital whether mother is married or not married at birth. gained weight gained by mother during pregnancy in pounds. weight weight of the baby at birth in pounds. lowbirthweight whether baby was classified as low birthweight (low) or not (not low). gender gender of the baby, female or male. habit status of the mother as a nonsmoker or a smoker. whitemom whether mom is white or not white. There are 1,000 cases in this data set, what do the cases represent? The hospitals where the births took place The fathers of the children The days of the births The births As a first step in the analysis, we should take a look at the variables in the dataset. This can be done using the str command: str(nc) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 1000 obs. of 13 variables: ## $ fage : int NA NA 19 21 NA NA 18 17 NA 20 ... ## $ mage : int 13 14 15 15 15 15 15 15 16 16 ... ## $ mature : Factor w/ 2 levels &quot;mature mom&quot;,&quot;younger mom&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ weeks : int 39 42 37 41 39 38 37 35 38 37 ... ## $ premie : Factor w/ 2 levels &quot;full term&quot;,&quot;premie&quot;: 1 1 1 1 1 1 1 2 1 1 ... ## $ visits : int 10 15 11 6 9 19 12 5 9 13 ... ## $ marital : Factor w/ 2 levels &quot;married&quot;,&quot;not married&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ gained : int 38 20 38 34 27 22 76 15 NA 52 ... ## $ weight : num 7.63 7.88 6.63 8 6.38 5.38 8.44 4.69 8.81 6.94 ... ## $ lowbirthweight: Factor w/ 2 levels &quot;low&quot;,&quot;not low&quot;: 2 2 2 2 2 1 2 1 2 2 ... ## $ gender : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 2 1 2 1 2 2 2 2 1 ... ## $ habit : Factor w/ 2 levels &quot;nonsmoker&quot;,&quot;smoker&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ whitemom : Factor w/ 2 levels &quot;not white&quot;,&quot;white&quot;: 1 1 2 2 1 1 1 1 2 2 ... As you review the variable summaries, consider which variables are categorical and which are numerical. For numerical variables, are there outliers? If you aren’t sure or want to take a closer look at the data, make a graph. 3.2 Exploratory data analysis We will first start with analyzing the weight gained by mothers throughout the pregnancy: gained. Using visualization and summary statistics, describe the distribution of weight gained by mothers during pregnancy. The summary function can also be useful. summary(nc$gained) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.00 20.00 30.00 30.33 38.00 85.00 27 How many mothers are we missing weight gain data from? 0 13 27 31 Next, consider the possible relationship between a mother’s smoking habit and the weight of her baby. Plotting the data is a useful first step because it helps us quickly visualize trends, identify strong associations, and develop research questions. Make side-by-side boxplots of habit and weight. Which of the following is false about the relationship between habit and weight? Median birth weight of babies born to non-smoker mothers is slightly higher than that of babies born to smoker mothers. Range of birth weights of babies born to non-smoker mothers is greater than that of babies born to smoker mothers. Both distributions are extremely right skewed. The IQRs of the distributions are roughly equal. # type your code for the Question 3 here, and Knit #boxplot(nc$habit,nc$weight) ggplot(nc, aes(x=habit, y=weight)) + geom_boxplot() The box plots show how the medians of the two distributions compare, but we can also compare the means of the distributions using the following to first group the data by the habit variable, and then calculate the mean weight in these groups using the mean function. nc %&gt;% group_by(habit) %&gt;% summarise(mean_weight = mean(weight)) ## # A tibble: 3 x 2 ## habit mean_weight ## &lt;fct&gt; &lt;dbl&gt; ## 1 nonsmoker 7.14 ## 2 smoker 6.83 ## 3 &lt;NA&gt; 3.63 There is an observed difference, but is this difference statistically significant? In order to answer this question we will conduct a hypothesis test. 3.3 Inference Exercise: Are all conditions necessary for inference satisfied? Comment on each. You can compute the group sizes using the same by command above but replacing mean(weight) with n(). Answer: This is the case of inference where two means are compared and t-distribution will be used. The categories are independent by definition and the number of samples are assumed to be &lt;10% of the entire population as there would be many number of cases. Both the group distributions are not very skewed as per the box plots above. nc %&gt;% group_by(habit) %&gt;% summarise(no_of_cases = n()) ## # A tibble: 3 x 2 ## habit no_of_cases ## &lt;fct&gt; &lt;int&gt; ## 1 nonsmoker 873 ## 2 smoker 126 ## 3 &lt;NA&gt; 1 What are the hypotheses for testing if the average weights of babies born to smoking and non-smoking mothers are different? \\(H_0: \\mu_{smoking} = \\mu_{non-smoking}\\); \\(H_A: \\mu_{smoking} &gt; \\mu_{non-smoking}\\) \\(H_0: \\mu_{smoking} = \\mu_{non-smoking}\\); \\(H_A: \\mu_{smoking} \\ne \\mu_{non-smoking}\\) \\(H_0: \\bar{x}_{smoking} = \\bar{x}_{non-smoking}\\); \\(H_A: \\bar{x}_{smoking} &gt; \\bar{x}_{non-smoking}\\) \\(H_0: \\bar{x}_{smoking} = \\bar{x}_{non-smoking}\\); \\(H_A: \\bar{x}_{smoking} &gt; \\bar{x}_{non-smoking}\\) \\(H_0: \\mu_{smoking} \\ne \\mu_{non-smoking}\\); \\(H_A: \\mu_{smoking} = \\mu_{non-smoking}\\) Next, we introduce a new function, inference, that we will use for conducting hypothesis tests and constructing confidence intervals. Then, run the following: inference(y = weight, x = habit, data = nc, statistic = &quot;mean&quot;, type = &quot;ht&quot;, null = 0, alternative = &quot;twosided&quot;, method = &quot;theoretical&quot;) ## Response variable: numerical ## Explanatory variable: categorical (2 levels) ## n_nonsmoker = 873, y_bar_nonsmoker = 7.1443, s_nonsmoker = 1.5187 ## n_smoker = 126, y_bar_smoker = 6.8287, s_smoker = 1.3862 ## H0: mu_nonsmoker = mu_smoker ## HA: mu_nonsmoker != mu_smoker ## t = 2.359, df = 125 ## p_value = 0.0199 Let’s pause for a moment to go through the arguments of this custom function. The first argument is y, which is the response variable that we are interested in: weight. The second argument is the explanatory variable, x, which is the variable that splits the data into two groups, smokers and non-smokers: habit. The third argument, data, is the data frame these variables are stored in. Next is statistic, which is the sample statistic we’re using, or similarly, the population parameter we’re estimating. In future labs we can also work with “median” and “proportion”. Next we decide on the type of inference we want: a hypothesis test (&quot;ht&quot;) or a confidence interval (&quot;ci&quot;). When performing a hypothesis test, we also need to supply the null value, which in this case is 0, since the null hypothesis sets the two population means equal to each other. The alternative hypothesis can be &quot;less&quot;, &quot;greater&quot;, or &quot;twosided&quot;. Lastly, the method of inference can be &quot;theoretical&quot; or &quot;simulation&quot; based. For more information on the inference function see the help file with ?inference. Exercise: What is the conclusion of the hypothesis test? Change the type argument to &quot;ci&quot; to construct and record a confidence interval for the difference between the weights of babies born to nonsmoking and smoking mothers, and interpret this interval in context of the data. Note that by default you’ll get a 95% confidence interval. If you want to change the confidence level, add a new argument (conf_level) which takes on a value between 0 and 1. Also note that when doing a confidence interval arguments like null and alternative are not useful, so make sure to remove them. We are 95% confident that babies born to nonsmoker mothers are on average 0.05 to 0.58 pounds lighter at birth than babies born to smoker mothers. We are 95% confident that the difference in average weights of babies whose moms are smokers and nonsmokers is between 0.05 to 0.58 pounds. We are 95% confident that the difference in average weights of babies in this sample whose moms are smokers and nonsmokers is between 0.05 to 0.58 pounds. We are 95% confident that babies born to nonsmoker mothers are on average 0.05 to 0.58 pounds heavier at birth than babies born to smoker mothers. # type your code for the Question 5 here, and Knit inference(y = weight, x = habit, data = nc, statistic = &quot;mean&quot;, type = &quot;ci&quot;, method = &quot;theoretical&quot;) ## Response variable: numerical, Explanatory variable: categorical (2 levels) ## n_nonsmoker = 873, y_bar_nonsmoker = 7.1443, s_nonsmoker = 1.5187 ## n_smoker = 126, y_bar_smoker = 6.8287, s_smoker = 1.3862 ## 95% CI (nonsmoker - smoker): (0.0508 , 0.5803) By default the function reports an interval for (\\(\\mu_{nonsmoker} - \\mu_{smoker}\\)) . We can easily change this order by using the order argument: inference(y = weight, x = habit, data = nc, statistic = &quot;mean&quot;, type = &quot;ci&quot;, method = &quot;theoretical&quot;, order = c(&quot;smoker&quot;,&quot;nonsmoker&quot;)) ## Response variable: numerical, Explanatory variable: categorical (2 levels) ## n_smoker = 126, y_bar_smoker = 6.8287, s_smoker = 1.3862 ## n_nonsmoker = 873, y_bar_nonsmoker = 7.1443, s_nonsmoker = 1.5187 ## 95% CI (smoker - nonsmoker): (-0.5803 , -0.0508) Calculate a 99% confidence interval for the average length of pregnancies (weeks). Note that since you’re doing inference on a single population parameter, there is no explanatory variable, so you can omit the x variable from the function. Which of the following is the correct interpretation of this interval? (38.1526 , 38.5168) (38.0892 , 38.5661) (6.9779 , 7.2241) (38.0952 , 38.5742) # type your code for Question 6 here, and Knit inference(y = weeks, data = nc, statistic = &quot;mean&quot;, type = &quot;ci&quot;, conf_level = 0.99, method = &quot;theoretical&quot;) ## Single numerical variable ## n = 998, y-bar = 38.3347, s = 2.9316 ## 99% CI: (38.0952 , 38.5742) Exercise: Calculate a new confidence interval for the same parameter at the 90% confidence level. Comment on the width of this interval versus the one obtained in the the previous exercise. Answer: This CI is narrower compared to the 99% CI. # type your code for the Exercise here, and Knit inference(y = weeks, data = nc, statistic = &quot;mean&quot;, type = &quot;ci&quot;, conf_level = 0.90, method = &quot;theoretical&quot;) ## Single numerical variable ## n = 998, y-bar = 38.3347, s = 2.9316 ## 90% CI: (38.1819 , 38.4874) Exercise: Conduct a hypothesis test evaluating whether the average weight gained by younger mothers is different than the average weight gained by mature mothers. # type your code for the Exercise here, and Knit #str(nc) inference(y = weight, x = mature, data = nc, statistic = &quot;mean&quot;, type = &quot;ht&quot;, method = &quot;theoretical&quot;, alternative = &quot;twosided&quot;, null = 0) ## Response variable: numerical ## Explanatory variable: categorical (2 levels) ## n_mature mom = 133, y_bar_mature mom = 7.1256, s_mature mom = 1.6591 ## n_younger mom = 867, y_bar_younger mom = 7.0972, s_younger mom = 1.4855 ## H0: mu_mature mom = mu_younger mom ## HA: mu_mature mom != mu_younger mom ## t = 0.1858, df = 132 ## p_value = 0.8529 Now, a non-inference task: Determine the age cutoff for younger and mature mothers. Use a method of your choice, and explain how your method works. # type your code for Question 7 here, and Knit #str(nc) nc %&gt;% group_by(mature) %&gt;% #blocking for maturity by this line of code summarise(min = min(mage), max = max(mage)) # summarizing the ages in a table ## # A tibble: 2 x 3 ## mature min max ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 mature mom 35 50 ## 2 younger mom 13 34 Exercise: Pick a pair of variables: one numerical (response) and one categorical (explanatory). Come up with a research question evaluating the relationship between these variables. Formulate the question in a way that it can be answered using a hypothesis test and/or a confidence interval. Answer your question using the inference function, report the statistical results, and also provide an explanation in plain language. Be sure to check all assumptions,state your \\(\\alpha\\) level, and conclude in context. (Note: Picking your own variables, coming up with a research question, and analyzing the data to answer this question is basically what you’ll need to do for your project as well.) # type your code for the Exercise here, and Knit This is a product of OpenIntro that is released under a Creative Commons Attribution-ShareAlike 3.0 Unported. This lab was written for OpenIntro by Andrew Bray and Mine Çetinkaya-Rundel. "],
["week4.html", "4 Week 4 4.1 Getting Started 4.2 Inference on proportions 4.3 How does the proportion affect the margin of error?", " 4 Week 4 Week 4 is about making inference from categorical data. Complete all Exercises, and submit answers to Questions on the Coursera platform. In August of 2012, news outlets ranging from the Washington Post to the Huffington Post ran a story about the rise of atheism in America. The source for the story was a poll that asked people, “Irrespective of whether you attend a place of worship or not, would you say you are a religious person, not a religious person or a convinced atheist?” This type of question, which asks people to classify themselves in one way or another, is common in polling and generates categorical data. In this lab we take a look at the atheism survey and explore what’s at play when making inference about population proportions using categorical data. 4.1 Getting Started 4.1.1 Load packages In this lab we will explore the data using the dplyr package and visualize it using the ggplot2 package for data visualization. The data can be found in the companion package for this course, statsr. Let’s load the packages. library(statsr) library(dplyr) library(ggplot2) 4.1.2 The survey The press release for the poll, conducted by WIN-Gallup International, can be accessed here. Take a moment to review the report then address the following questions. How many people were interviewed for this survey? A poll conducted by WIN-Gallup International surveyed 51,000 people from 57 countries. A poll conducted by WIN-Gallup International surveyed 52,000 people from 57 countries. A poll conducted by WIN-Gallup International surveyed 51,917 people from 57 countries. A poll conducted by WIN-Gallup International surveyed 51,927 people from 57 countries. Which of the following methods were used to gather information? Face to face Telephone Internet All of the above True / False: In the first paragraph, several key findings are reported. These percentages appear to be sample statistics. True False True / False:The title of the report is “Global Index of Religiosity and Atheism”. To generalize the report’s findings to the global human population, We must assume that the sample was a random sample from the entire population in order to be able to generalize the results to the global human population. This seems to be a reasonable assumption. True False 4.1.3 The data Turn your attention to Table 6 (pages 15 and 16), which reports the sample size and response percentages for all 57 countries. While this is a useful format to summarize the data, we will base our analysis on the original data set of individual responses to the survey. Load this data set into R with the following command. data(atheism) What does each row of Table 6 correspond to? Countries Individual Persons Religions What does each row of atheism correspond to? Countries Individual Persons Religions To investigate the link between these two ways of organizing this data, take a look at the estimated proportion of atheists in the United States. Towards the bottom of Table 6, we see that this is 5%. We should be able to come to the same number using the atheism data. Create a new dataframe called us12 that contains only the rows in atheism associated with respondents to the 2012 survey from the United States: us12 &lt;- atheism %&gt;% filter(nationality == &quot;United States&quot; , atheism$year == &quot;2012&quot;) Next, calculate the proportion of atheist responses in the United States in 2012, i.e. in us12. True / False: This percentage agrees with the percentage in Table~6. True False # type your code for Question 7 here, and Knit us12 %&gt;% group_by(response) %&gt;% summarise(n = n()) %&gt;% # Show percentage of each level within the categorical variable mutate(perc = n/sum(n)) ## # A tibble: 2 x 3 ## response n perc ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 atheist 50 0.0499 ## 2 non-atheist 952 0.950 4.2 Inference on proportions As was hinted earlier, Table 6 provides sample statistics, that is, calculations made from the sample of 51,927 people. What we’d like, though, is insight into the population population parameters. You answer the question, “What proportion of people in your sample reported being atheists?” with a statistic; while the question “What proportion of people on earth would report being atheists” is answered with an estimate of the parameter. The inferential tools for estimating population proportion are analogous to those used for means in the last lab: the confidence interval and the hypothesis test. Exercise: Write out the conditions for inference to construct a 95% confidence interval for the proportion of atheists in the United States in 2012. Are you confident all conditions are met? If the conditions for inference are reasonable, we can either calculate the standard error and construct the interval by hand, or allow the inference function to do it for us. inference(y = response, data = us12, statistic = &quot;proportion&quot;, type = &quot;ci&quot;, method = &quot;theoretical&quot;, success = &quot;atheist&quot;) ## Single categorical variable, success: atheist ## n = 1002, p-hat = 0.0499 ## 95% CI: (0.0364 , 0.0634) Note that since the goal is to construct an interval estimate for a proportion, it’s necessary to specify what constitutes a `success'', which here is a response ofatheist`. Although formal confidence intervals and hypothesis tests don’t show up in the report, suggestions of inference appear at the bottom of page 7: “In general, the error margin for surveys of this kind is \\(\\pm\\) 3-5% at 95% confidence.” Exercise: Imagine that, after reading a front page story about the latest public opinion poll, a family member asks you, “What is a margin of error?” In one sentence, and ignoring the mechanics behind the calculation, how would you respond in a way that conveys the general concept? Answer: To represent the entire population there will be an offset from the point estimate either on the positive or the negative side based on the confidence level we need. Based on the R output, what is the margin of error for the estimate of the proportion of the proportion of atheists in US in 2012? The margin of error for the estimate of the proportion of atheists in the US in 2012 is 0.05. The margin of error for the estimate of the proportion of atheists in the US in 2012 is 0.025. The margin of error for the estimate of the proportion of atheists in the US in 2012 is 0.0135. # type your code for Question 8 here, and Knit n &lt;- 1002 p_hat &lt;- 0.0499 se &lt;- sqrt(p_hat*(1-(p_hat))/n) moe &lt;- qnorm(0.975)*se moe ## [1] 0.01348184 Exercise: Using the inference function, calculate confidence intervals for the proportion of atheists in 2012 in two other countries of your choice, and report the associated margins of error. Be sure to note whether the conditions for inference are met. It may be helpful to create new data sets for each of the two countries first, and then use these data sets in the inference function to construct the confidence intervals. # type your code for the Exercise here, and Knit 4.3 How does the proportion affect the margin of error? Imagine you’ve set out to survey 1000 people on two questions: are you female? and are you left-handed? Since both of these sample proportions were calculated from the same sample size, they should have the same margin of error, right? Wrong! While the margin of error does change with sample size, it is also affected by the proportion. Think back to the formula for the standard error: \\(SE = \\sqrt{p(1-p)/n}\\). This is then used in the formula for the margin of error for a 95% confidence interval: \\(ME = 1.96\\times SE = 1.96\\times\\sqrt{p(1-p)/n}\\). Since the population proportion \\(p\\) is in this \\(ME\\) formula, it should make sense that the margin of error is in some way dependent on the population proportion. We can visualize this relationship by creating a plot of \\(ME\\) vs. \\(p\\). The first step is to make a vector p that is a sequence from \\(0\\) to \\(1\\) with each number separated by \\(0.01\\). We can then create a vector of the margin of error (me) associated with each of these values of p using the familiar approximate formula (\\(ME = 1.96 \\times SE\\)). Lastly, we plot the two vectors against each other to reveal their relationship. d &lt;- data.frame(p &lt;- seq(0, 1, 0.01)) n &lt;- 1000 d &lt;- d %&gt;% mutate(me = 1.96*sqrt(p*(1 - p)/n)) ggplot(d, aes(x = p, y = me)) + geom_line() Which of the following is false about the relationship between \\(p\\) and \\(ME\\). The \\(ME\\) reaches a minimum at \\(p = 0\\). The \\(ME\\) reaches a minimum at \\(p = 1\\). The \\(ME\\) is maximized when \\(p = 0.5\\). The most conservative estimate when calculating a confidence interval occurs when \\(p\\) is set to 1. The question of atheism was asked by WIN-Gallup International in a similar survey that was conducted in 2005. We assume here that sample sizes have remained the same. Table 4 on page 13 of the report summarizes survey results from 2005 and 2012 for 39 countries. Answer the following two questions using the inference function. As always, write out the hypotheses for any tests you conduct and outline the status of the conditions for inference. True / False: There is convincing evidence that Spain has seen a change in its atheism index between 2005 and 2012. Hint: Create a new data set for respondents from Spain. Then use their responses as the first input on the inference, and use year as the grouping variable. True False # type your code for Question 10 here, and Knit spain &lt;- atheism %&gt;% filter(nationality == &quot;Spain&quot;) %&gt;% group_by(year) inference(y = response, x = as.factor(year), data = spain, statistic = &quot;proportion&quot;, method = &quot;theoretical&quot;, type = &quot;ht&quot;, success = &quot;atheist&quot;, alternative = &quot;twosided&quot;, null = 0 ) ## Response variable: categorical (2 levels, success: atheist) ## Explanatory variable: categorical (2 levels) ## n_2005 = 1146, p_hat_2005 = 0.1003 ## n_2012 = 1145, p_hat_2012 = 0.09 ## H0: p_2005 = p_2012 ## HA: p_2005 != p_2012 ## z = 0.8476 ## p_value = 0.3966 True / False: There is convincing evidence that the United States has seen a change in its atheism index between 2005 and 2012. True False # type your code for Question 11 here, and Knit us &lt;- atheism %&gt;% filter(nationality == &quot;United States&quot;) inference(y = response, data = us, x = as.factor(year), method = &quot;theoretical&quot;, type = &quot;ht&quot;, alternative = &quot;twosided&quot;, null = 0, statistic = &quot;proportion&quot;, success = &quot;atheist&quot;) ## Response variable: categorical (2 levels, success: atheist) ## Explanatory variable: categorical (2 levels) ## n_2005 = 1002, p_hat_2005 = 0.01 ## n_2012 = 1002, p_hat_2012 = 0.0499 ## H0: p_2005 = p_2012 ## HA: p_2005 != p_2012 ## z = -5.2431 ## p_value = &lt; 0.0001 If in fact there has been no change in the atheism index in the countries listed in Table 4, in how many of those countries would you expect to detect a change (at a significance level of 0.05) simply by chance? Hint: Type 1 error. 0 1 1.95 5 # type your code for Question 12 here, and Knit sig_level = 0.05 nt4 = 39 Type_1_error &lt;- sig_level * nt4 Type_1_error ## [1] 1.95 Suppose you’re hired by the local government to estimate the proportion of residents that attend a religious service on a weekly basis. According to the guidelines, the estimate must have a margin of error no greater than 1% with 95% confidence. You have no idea what to expect for \\(p\\). How many people would you have to sample to ensure that you are within the guidelines? Hint: Refer to your plot of the relationship between \\(p\\) and margin of error. Do not use the data set to answer this question. 2401 people At least 2401 people 9604 people At least 9604 people # type your code for Question 13 here, and Knit p &lt;- 0.5 z_crit &lt;- qnorm(0.975) n_sample &lt;- z_crit^2 * p*(1-p)/0.01^2 n_sample ## [1] 9603.647 This is a product of OpenIntro that is released under a Creative Commons Attribution-ShareAlike 3.0 Unported. This lab was written for OpenIntro by Andrew Bray and Mine Çetinkaya-Rundel. "],
["week5.html", "5 Week 5 5.1 Setup 5.2 Part 1: Data 5.3 Part 2: Research question 5.4 Part 3: Exploratory data analysis 5.5 Part 4: Inference", " 5 Week 5 Week 5 entails a mini-project which covers framing the research question, doing exploratory data analysis and answering the research question with the help of statistical inference. 5.1 Setup 5.1.1 Load packages library(ggplot2) library(dplyr) library(statsr) library(vcd) 5.1.2 Load data load(&quot;gss.Rdata&quot;) 5.2 Part 1: Data Describe how the observations in the sample are collected, and the implications of this data collection method on the scope of inference (generalizability / causality)? The data was collected by full probability sampling commonly called random sampling across years by helding personal interviews. Within this, there were stratified samplings by different parameters like age, region, race, income. This is an interview survey so it is voluntary and nothing is controlled which makes it an observational study. This can be generalized to the United States population and statistical significance can be found to know the extent to which the samples’ data is representative of the entire US population. Causal inference can’t be derived as there is no random assignment rather there are observations of independently and randomly made samples. There is also non-response subsampling which would help in reducing the non-response bias. Reducing this bias assures the statistical significance and makes the chosen samples more representative of the entire US population. 5.3 Part 2: Research question Is there an association between race of the respondent and natrace (an attitudinal measure about Improving the conditions of blacks)? natrace has 3 levels: Too Little, About Right, Too Much This is of interest because: The association if it exists helps us understand to further engineer the variables and create new variables for predictive models. More importantly, the statistical inference within the sample represents the entire population which otherwise would not be known. This is also for a quantitative find about how the race of a person makes the reply to this question of interest in the variable natrace. 5.4 Part 3: Exploratory data analysis #to check the variable names #str(gss) #filtering out null values conting_sb &lt;- gss %&gt;% filter(!is.na(natrace), !is.na(race)) #segmented bar plot ggplot(conting_sb, aes(x = race)) + geom_bar(aes(fill = natrace), position = &#39;fill&#39;) + labs(x = &quot;race&quot;, y = &quot;proportion of natrace response&quot;) Narrative: The proportion of blacks who has the opinion that ‘Too little’ is spent on improving the conditions of blacks is much higher compared to ‘Whites’ and Other races. This matched my intuition. To understand if this is statistically significant, we will find out the association between these variables using chi-square test of independence. There is mosaic plot too coming up which best represents the contingency table. 5.5 Part 4: Inference Hypotheses: H0: There is no association between the categorical variables race and natrace. HA: There is a relationship between the categorical variables race and natrace. #creating a contingency table ct &lt;- table(gss$race, gss$natrace) ct ## ## Too Little About Right Too Much ## White 6532 12447 5884 ## Black 3470 772 78 ## Other 456 525 145 Conditions: Independence: Random sampling of the entire sample was mentioned in the GSS study. When it comes to each group, by seeing the contingency table above, the number of cases are definitely less than 10% of respective populations. The respective populations would be way higher. Each case definitely contributed to one specific case and to remove any ambiguity the NA values have been filtered out. Sample size: Each particular cell has greater than or equal to 5 cases as per the contingency table. cont &lt;- as.table(as.matrix(ct)) mosaicplot(cont, shade = TRUE, las=2, main = &quot;Mosaic Plot between race and natrace&quot;) Higher standardized residuals means higher frequency of cases indicated by blue color boxes. Majority of whites are categorized under ‘About Right’ or ‘Too Much’ category while majortiy of blacks are categorized under ‘Too Little’ Method to be used and why and how: We will be using chi-square test of independence to find out the relationship between these two categorical variables. Both the variables have more than 2 categorical levels and the conditions for the test of independence (TOI) are satisfied. # Calculating chi-square statistic and the corresponding p-value chisq_race_natrace &lt;- chisq.test(ct) chisq_race_natrace ## ## Pearson&#39;s Chi-squared test ## ## data: ct ## X-squared = 4873.7, df = 4, p-value &lt; 2.2e-16 # expected counts in each cell of the contingency table round(chisq_race_natrace$expected,2) ## ## Too Little About Right Too Much ## White 8578.88 11274.44 5009.68 ## Black 1490.60 1958.96 870.44 ## Other 388.52 510.60 226.88 Results: Based on the high value of chi-square statistic and the degrees of freedom under consideration, we have a p-value which is very tiny compared to the 5% significance level. This implies there is a clear association/ relationship between the variables race and natrace. Further analysis can be carried out to find out if it is a positive correlation or a negative correlation. Hoping to learn that soon. The confidence interval calcualtion is not included as it not associated with chi-square testing and that is because chi-square distribution is always right skewed. New techniques have to be implemented to make the distribution normal and then CLT or confidence interval can be applied. Created by: Akshay Kotha "]
]
