--- 
title: "Inferential Statistics Course Activity"
author: "Akshay Kotha"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: akshaykumarreddy/stat-inference-proj
description: "This is the course activity week-by-week for the inferential statistics course on coursera."
---
# Preface {-}

Check out the list of weeks and the project to the left.

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:index.Rmd-->

# Week 1 {#week1}

Week 1 covers calculating sampling distributions. Below is the activity.

---
title: "Foundations for inference - Sampling distributions"
output: statsr:::statswithr_lab
---

<div id="instructions">
Complete all **Exercises**, and submit answers to **Questions** on the Coursera 
platform.
</div>

## Getting Started

### Load packages

In this lab we will explore the data using the `dplyr` package and visualize it 
using the `ggplot2` package for data visualization. The data can be found in the
companion package for this course, `statsr`.

Let's load the packages.

```{r load-packages, message=FALSE}
library(statsr)
library(dplyr)
library(shiny)
library(ggplot2)
```

### The data

We consider real estate data from the city of Ames, Iowa. The details of 
every real estate transaction in Ames is recorded by the City Assessor's 
office. Our particular focus for this lab will be all residential home sales 
in Ames between 2006 and 2010.  This collection represents our population of 
interest. In this lab we would like to learn about these home sales by taking 
smaller samples from the full population. Let's load the data.

```{r load-data}
data(ames)
```

We see that there are quite a few variables in the data set, enough to do a 
very in-depth analysis. For this lab, we'll restrict our attention to just 
two of the variables: the above ground living area of the house in square feet 
(`area`) and the sale price (`price`).

We can explore the distribution of areas of homes in the population of home
sales visually and with summary statistics. Let's first create a visualization,
a histogram:

```{r area-hist}
ggplot(data = ames, aes(x = area)) +
  geom_histogram(binwidth = 250)
```

Let's also obtain some summary statistics. Note that we can do this using the
`summarise` function. We can calculate as many statistics as we want using this
function, and just string along the results. Some of the functions below should
be self explanatory (like `mean`, `median`, `sd`, `IQR`, `min`, and `max`). A
new function here is the `quantile` function which we can use to calculate 
values corresponding to specific percentile cutoffs in the distribution. For
example `quantile(x, 0.25)` will yield the cutoff value for the 25th percentile (Q1)
in the distribution of x. Finding these values are useful for describing the 
distribution, as we can use them for descriptions like *"the middle 50% of the 
homes have areas between such and such square feet"*.

```{r area-stats}
ames %>%
  summarise(mu = mean(area), pop_med = median(area), 
            sigma = sd(area), pop_iqr = IQR(area),
            pop_min = min(area), pop_max = max(area),
            pop_q1 = quantile(area, 0.25),  # first quartile, 25th percentile
            pop_q3 = quantile(area, 0.75))  # third quartile, 75th percentile
```

1.  Which of the following is **false**?
<ol>
<li> The distribution of areas of houses in Ames is unimodal and right-skewed. </li>
<li> 50\% of houses in Ames are smaller than 1,499.69 square feet. </li>
<li> The middle 50\% of the houses range between approximately 1,126 square feet and 1,742.7 square feet. </li>
<li> The IQR is approximately 616.7 square feet. </li>
<li> The smallest house is 334 square feet and the largest is 5,642 square feet. </li>
</ol>


## The unknown sampling distribution

In this lab we have access to the entire population, but this is rarely the 
case in real life. Gathering information on an entire population is often 
extremely costly or impossible. Because of this, we often take a sample of 
the population and use that to understand the properties of the population.

If we were interested in estimating the mean living area in Ames based on a 
sample, we can use the following command to survey the population.

```{r samp1}
samp1 <- ames %>%
  sample_n(size = 50)
```

This command collects a simple random sample of `size` 50 from the `ames` dataset, 
which is assigned to `samp1`. This is like going into the City 
Assessor's database and pulling up the files on 50 random home sales. Working 
with these 50 files would be considerably simpler than working with all 2930 
home sales.

<div id="exercise">
**Exercise**: Describe the distribution of this sample? How does it compare to the distribution of the population? **Hint:** `sample_n` function takes a random sample of observations (i.e. rows) from the dataset, you can still refer to the variables in the dataset with the same names. Code you used in the previous exercise will also be helpful for visualizing and summarizing the sample, however be careful to not label values `mu` and `sigma` anymore since these are sample statistics, not population parameters. You can customize the labels of any of the statistics to indicate that these come from the sample.
</div>
```{r samp1-dist}
# type your code for the Exercise here, and Run Document
ggplot(samp1, aes(x = area)) + geom_histogram(binwidth = 200)

# The distribution is symmetric unlike the population distribution
```


If we're interested in estimating the average living area in homes in Ames 
using the sample, our best single guess is the sample mean.

```{r mean-samp1}
samp1 %>%
  summarise(x_bar = mean(area))
```

Depending on which 50 homes you selected, your estimate could be a bit above 
or a bit below the true population mean of 1,499.69 square feet. In general, 
though, the sample mean turns out to be a pretty good estimate of the average 
living area, and we were able to get it by sampling less than 3\% of the 
population.

2. Suppose we took two more samples, one of size 100 and one of size 1000. Which would you think would provide a more accurate estimate of the population mean?
<ol>
<li> Sample size of 50. </li>
<li> Sample size of 100. </li>
<li> Sample size of 1000. </li>
</ol>

Let's take one more sample of size 50, and view the mean area in this sample:
```{r mean-samp2}
ames %>%
  sample_n(size = 1000) %>%
  summarise(x_bar = mean(area))
```

Not surprisingly, every time we take another random sample, we get a different 
sample mean. It's useful to get a sense of just how much variability we 
should expect when estimating the population mean this way. The distribution 
of sample means, called the *sampling distribution*, can help us understand 
this variability. In this lab, because we have access to the population, we 
can build up the sampling distribution for the sample mean by repeating the 
above steps many times. Here we will generate 15,000 samples and compute the 
sample mean of each. Note that we are sampling with replacement, 
`replace = TRUE` since sampling distributions are constructed with sampling
with replacement.

```{r loop}
sample_means50 <- ames %>%
                    rep_sample_n(size = 50, reps = 15000, replace = TRUE) %>%
                    summarise(x_bar = mean(area))

ggplot(data = sample_means50, aes(x = x_bar)) +
  geom_histogram(binwidth = 20)
```

Here we use R to take 15,000 samples of size 50 from the population, calculate 
the mean of each sample, and store each result in a vector called 
`sample_means50`. Next, we review how this set of code works.

<div id="exercise">
**Exercise**: How many elements are there in `sample_means50`?  Describe the sampling distribution, and be sure to specifically note its center. Make sure to include a plot of the distribution in your answer.
</div>
```{r sampling-dist}
# type your code for the Exercise here, and Run Document

#No. of elements in sample_means50
dim(sample_means50)

# Center of sampling distribution
sample_means50 %>%
  summarise(means = mean(x_bar))

#Given that 15000 samples are considered, the distribution resembles a normal distribution and it looks symmetric. Sample means vary around the population mean equally.
```

## Interlude: Sampling distributions

The idea behind the `rep_sample_n` function is *repetition*. Earlier we took
a single sample of size `n` (50) from the population of all houses in Ames. With
this new function we are able to repeat this sampling procedure `rep` times in order
to build a distribution of a series of sample statistics, which is called the 
**sampling distribution**. 

Note that in practice one rarely gets to build sampling distributions, 
because we rarely have access to data from the entire population. 

Without the `rep_sample_n` function, this would be painful. We would have to 
manually run the following code 15,000 times 
```{r sample-code, eval=FALSE}
ames %>%
  sample_n(size = 50) %>%
  summarise(x_bar = mean(area))
```
as well as store the resulting sample means each time in a separate vector.

Note that for each of the 15,000 times we computed a mean, we did so from a 
**different** sample!

<div id="exercise">
**Exercise**: To make sure you understand how sampling distributions are built, and exactly what the `sample_n` and `do` function do, try modifying the code to create a sampling distribution of **25 sample means** from **samples of size 10**, and put them in a data frame named `sample_means_small`. Print the output. How many observations are there in this object called `sample_means_small`? What does each observation represent?
</div>
```{r practice-sampling-dist}
# type your code for the Exercise here, and Run Document
sample_means_small <- ames %>%
                    rep_sample_n(size = 10, reps = 25, replace = TRUE) %>%
                    summarise(x_bar_practice = mean(area))


```

3. How many elements are there in this object called `sample_means_small`? 
<ol>
<li> 0 </li>
<li> 3 </li>
<li> 25 </li>
<li> 100 </li>
<li> 5,000 </li>
</ol>
```{r sample-means-small}
# type your code for Question 3 here, and Run Document
dim(sample_means_small)
sample_means_small
```

4. Which of the following is **true** about the elements in the sampling distributions you created?
<ol>
<li> Each element represents a mean square footage from a simple random sample of 10 houses. </li>
<li> Each element represents the square footage of a house. </li>
<li> Each element represents the true population mean of square footage of houses. </li>
</ol>


## Sample size and the sampling distribution

Mechanics aside, let's return to the reason we used the `rep_sample_n` function: to 
compute a sampling distribution, specifically, this one.

```{r hist}
ggplot(data = sample_means50, aes(x = x_bar)) +
  geom_histogram(binwidth = 20)
```

The sampling distribution that we computed tells us much about estimating 
the average living area in homes in Ames.  Because the sample mean is an 
unbiased estimator, the sampling distribution is centered at the true average 
living area of the population, and the spread of the distribution 
indicates how much variability is induced by sampling only 50 home sales.

In the remainder of this section we will work on getting a sense of the effect that 
sample size has on our sampling distribution.

<div id="exercise">
**Exercise**: Use the app below to create sampling distributions of means of `area`s from samples of size 10, 50, and 100. Use 5,000 simulations. What does each observation in the sampling distribution represent? How does the mean, standard error, and shape of the sampling distribution change as the sample size increases? How (if at all) do these values change if you increase the number of simulations?
</div>
    
```{r shiny, echo=FALSE}
shinyApp(
  ui <- fluidPage(
    
    # Sidebar with a slider input for number of bins 
    sidebarLayout(
      sidebarPanel(
        
        selectInput("selected_var",
                    "Variable:",
                    choices = list("area", "price"),
                    selected = "area"),         
        
        numericInput("n_samp",
                     "Sample size:",
                     min = 1,
                     max = nrow(ames),
                     value = 30),
        
        numericInput("n_sim",
                     "Number of samples:",
                     min = 1,
                     max = 30000,
                     value = 15000) 
        
      ),
      
      # Show a plot of the generated distribution
      mainPanel(
        plotOutput("sampling_plot"),
        verbatimTextOutput("sampling_mean"),
        verbatimTextOutput("sampling_se")
      )
    )
  ),
  
  # Define server logic required to draw a histogram
  server <- function(input, output) {
    
    # create sampling distribution
    sampling_dist <- reactive({
      ames[[input$selected_var]] %>%
        sample(size = input$n_samp * input$n_sim, replace = TRUE) %>%
        matrix(ncol = input$n_samp) %>%
        rowMeans() %>%
        data.frame(x_bar = .)
      #ames %>%
      #  rep_sample_n(size = input$n_samp, reps = input$n_sim, replace = TRUE) %>%
      #  summarise_(x_bar = mean(input$selected_var))
    })
    
    # plot sampling distribution
    output$sampling_plot <- renderPlot({
      x_min <- quantile(ames[[input$selected_var]], 0.1)
      x_max <- quantile(ames[[input$selected_var]], 0.9)
      
      ggplot(sampling_dist(), aes(x = x_bar)) +
        geom_histogram() +
        xlim(x_min, x_max) +
        ylim(0, input$n_sim * 0.35) +
        ggtitle(paste0("Sampling distribution of mean ", 
                       input$selected_var, " (n = ", input$n_samp, ")")) +
        xlab(paste("mean", input$selected_var)) +
        theme(plot.title = element_text(face = "bold", size = 16))
    })
    
    # mean of sampling distribution
    output$sampling_mean <- renderText({
      paste0("mean of sampling distribution = ", round(mean(sampling_dist()$x_bar), 2))
    })
    
    # mean of sampling distribution
    output$sampling_se <- renderText({
      paste0("SE of sampling distribution = ", round(sd(sampling_dist()$x_bar), 2))
    })
  },
  
  options = list(height = 500) 
)
```

5. It makes intuitive sense that as the sample size increases, the center of the sampling distribution becomes a more reliable estimate for the true population mean. Also as the sample size increases, the variability of the sampling distribution ________. 
<ol>
<li> decreases </li>
<li> increases </li>
<li> stays the same </li>
</ol>

<div id="exercise">
**Exercise**: Take a random sample of size 50 from `price`. Using this sample, what is your best point estimate of the population mean?
</div>
```{r price-sample}
# type your code for this Exercise here, and Run Document

samp1 %>%
  summarise(mu_price = mean(price))
```

<div id="exercise">
**Exercise**: Since you have access to the population, simulate the sampling distribution for $\bar{x}_{price}$ by taking 5000 samples from the population of size 50 and computing 5000 sample means.  Store these means in a vector called `sample_means50`. Plot the data, then describe the shape of this sampling distribution. Based on this sampling distribution, what would you guess the mean home price of the population to be?
</div>
```{r price-sampling}
# type your code for this Exercise here, and Run Document
sample_means50 <- ames %>%
  rep_sample_n(size = 50, reps  = 5000, replace =  TRUE) %>%
  summarise(x_bar_price = mean(price))

ggplot(sample_means50,aes(x = x_bar_price)) + geom_histogram(binwidth = 1000)
```

<div id="exercise">
**Exercise**: Change your sample size from 50 to 150, then compute the sampling distribution using the same method as above, and store these means in a new vector called `sample_means150`. Describe the shape of this sampling distribution, and compare it to the sampling distribution for a sample size of 50.  Based on this sampling distribution, what would you guess to be the mean sale price of homes in Ames?
</div>
```{r price-sampling-more}
# type your code for this Exercise here, and Run Document
sample_means150 <- ames %>%
  rep_sample_n(size = 150, reps  = 5000, replace =  TRUE) %>%
  summarise(x_bar_price = mean(price))

ggplot(sample_means150,aes(x = x_bar_price)) + geom_histogram(binwidth = 1000)

```

* * *

So far, we have only focused on estimating the mean living area in homes in 
Ames. Now you'll try to estimate the mean home price.

Note that while you might be able to answer some of these questions using the app
you are expected to write the required code and produce the necessary plots and
summary statistics. You are welcomed to use the app for exploration.

<div id="exercise">
**Exercise**: Take a sample of size 15 from the population and calculate the mean `price` of the homes in this sample. Using this sample, what is your best point estimate of the population mean of prices of homes?
</div>
```{r price-sample-small}
# type your code for this Exercise here, and Run Document
samp1price <- ames %>%
  sample_n(size = 15) 

samp1price %>%
  summarise(mean(price))

```

<div id="exercise">
**Exercise**: Since you have access to the population, simulate the sampling distribution for $\bar{x}_{price}$ by taking 2000 samples from the population of size 15 and computing 2000 sample means. Store these means in a vector called `sample_means15`. Plot the data, then describe the shape of this sampling distribution. Based on this sampling distribution, what would you guess the mean home price of the population to be? Finally, calculate and report the population mean.
</div>
```{r price-sampling-small}
# type your code for this Exercise here, and Run Document
sample_means15 <- ames %>%
  rep_sample_n(size = 15, reps  = 2000, replace =  TRUE) %>%
  summarise(x_bar_price15 = mean(price))

ggplot(sample_means15,aes(x = x_bar_price15)) + geom_histogram(binwidth = 1000)

ames %>%
  summarise(mu_price = mean(price))

```

<div id="exercise">
**Exercise**: Change your sample size from 15 to 150, then compute the sampling distribution using the same method as above, and store these means in a new vector called `sample_means150`. Describe the shape of this sampling distribution, and compare it to the sampling distribution for a sample size of 15. Based on this sampling distribution, what would you guess to be the mean sale price of homes in Ames?
</div>
```{r price-sampling-big}
# type your code for this Exercise here, and Run Document
sample_means150 <- ames %>%
  rep_sample_n(size = 150, reps  = 2000, replace =  TRUE) %>%
  summarise(x_bar_price150 = mean(price))

ggplot(sample_means150,aes(x = x_bar_price150)) + geom_histogram(binwidth = 1000)
```

6. Which of the following is false? 
<ol>
<li> The variability of the sampling distribution with the smaller sample size (`sample_means50`) is smaller than the variability of the sampling distribution with the larger sample size (`sample_means150`). </li>
<li> The means for the two sampling distribtuions are roughly similar. </li> 
<li> Both sampling distributions are symmetric. </li>
</ol>
```{r price-sampling-compare}
# type your code for Question 6 here, and Run Document
# Comparing variability between two sampling distributions (n=50), (n =150)
sample_means50 %>%
  summarise( max(x_bar_price)-min(x_bar_price))

sample_means150 %>%
  summarise(max(x_bar_price150) - min(x_bar_price150))
```

<div id="license">
This is a derivative of an [OpenIntro](https://www.openintro.org/stat/labs.php) lab, and is released under a [Attribution-NonCommercial-ShareAlike 3.0 United States](https://creativecommons.org/licenses/by-nc-sa/3.0/us/) license.
</div>

<!--chapter:end:01_sampling_distributions_Coursera.Rmd-->

# Week 2 {#week2}

Week 2 covers calculating confidence intervals. Below is the activity.

---
title: "Foundations for inference - Confidence intervals"
output: statsr:::statswithr_lab
---

<div id="instructions">
Complete all **Exercises**, and submit answers to **Questions** on the Coursera 
platform.
</div>

If you have access to data on an entire population, say the size of every 
house in Ames, Iowa, it's straight forward to answer questions like, "How big 
is the typical house in Ames?" and "How much variation is there in sizes of 
houses?". If you have access to only a sample of the population, as is often 
the case, the task becomes more complicated. What is your best guess for the 
typical size if you only know the sizes of several dozen houses? This sort of 
situation requires that you use your sample to make inference on what your 
population looks like.

<div id="boxedtext">
**Setting a seed:** We will take some random samples and calculate confidence based
on these samples in this lab, which means you should set a seed on top of your lab. If 
this concept is new to you, review the previous lab and ask your TA.

Setting a seed will cause R to sample the same sample each time you knit your document.
This will make sure your results don't change each time you knit, and it will also 
ensure reproducibility of your work (by setting the same seed it will be possible to 
reproduce your results). You can set a seed like this:
```{r set-seed}
set.seed(19122018)                 # my seed
```
The number above is completely arbitraty. If you need inspiration, you can use your
ID, birthday, or just a random string of numbers. The important thing is that you
use each seed only once. You only need to do this once in your R Markdown document,
but make sure it comes before sampling.
</div>


## Getting Started

### Load packages

In this lab we will explore the data using the `dplyr` package and visualize it 
using the `ggplot2` package for data visualization. The data can be found in the
companion package for this course, `statsr`.

Let's load the packages.

```{r load-packages, message=FALSE}
library(statsr)
library(dplyr)
library(ggplot2)
```

### The data

We consider real estate data from the city of Ames, Iowa. This is the same 
dataset used in the previous lab. The details of 
every real estate transaction in Ames is recorded by the City Assessor's 
office. Our particular focus for this lab will be all residential home sales 
in Ames between 2006 and 2010.  This collection represents our population of 
interest. In this lab we would like to learn about these home sales by taking 
smaller samples from the full population. Let's load the data.

```{r load-data}
data(ames)
```

In this lab we'll start with a simple random sample of size 60 from the 
population. Specifically, this is a simple random sample of size 60. Note that 
the data set has information on many housing variables, but for the first 
portion of the lab we'll focus on the size of the house, represented by the 
variable `area`.

```{r sample}
n <- 60
samp <- sample_n(ames, n)
```

<div id="exercise">
**Exercise**: Describe the distribution of homes in your sample. What would you 
say is the "typical" size within your sample? Also state precisely what you 
interpreted "typical" to mean.
</div>
<div id="answer1">
**Answer**: Typical size between 1000 and 1500 as it seems to be a bi-modal distribution.'Typical' meaning size which is more frequent.
</div>
```{r describe-sample}
# type your code for the Exercise here, and Knit
ggplot(samp, aes(x = area)) + geom_histogram(binwidth = 320)

```

1. True or False: My distribution should be similar to others' distributions who also collect random samples from this population, but it is likely not exactly the same since it's a random sample.
<ol>
<li> True. </li>
<li> False. </li>
</ol>


## Confidence intervals

Return for a moment to the question that first motivated this lab: based on 
this sample, what can we infer about the population? Based only on this single 
sample, the best estimate of the average living area of houses sold in Ames 
would be the sample mean, usually denoted as $\bar{x}$ (here we're calling it 
`x_bar`). That serves as a good **point estimate** but it would be useful 
to also communicate how uncertain we are of that estimate. This uncertainty
can be quantified using a **confidence interval**.

A confidence interval for a population mean is of the following form
\[ \bar{x} \pm z^\star \frac{s}{\sqrt{n}} \]

You should by now be comfortable with calculating the mean and standard deviation of 
a sample in R. And we know that the sample size is 60. So the only remaining building
block is finding the appropriate critical value for a given confidence level. We can
use the `qnorm` function for this task, which will give the critical value associated
with a given percentile under the normal distribution. Remember that confidence levels
and percentiles are not equivalent. For example, a 95% confidence level refers to the
middle 95% of the distribution, and the critical value associated with this area will
correspond to the 97.5th percentile.

We can find the critical value for a 95% confidence interal using
```{r z_star_95, cache = TRUE}
z_star_95 <- qnorm(0.975)
z_star_95
```
which is roughly equal to the value critical value 1.96 that you're likely
familiar with by now.

Let's finally calculate the confidence interval:
```{r ci}
samp %>%
  summarise(lower = mean(area) - z_star_95 * (sd(area) / sqrt(n)),
            upper = mean(area) + z_star_95 * (sd(area) / sqrt(n)))
```

To recap: even though we don't know what the full population looks like, we're 95% 
confident that the true average size of houses in Ames lies between the values *lower* 
and *upper*. There are a few conditions that must be met for this interval to be valid.

2.  For the confidence interval to be valid, the sample mean must be normally distributed and have standard error $s / \sqrt{n}$. Which of the following is not a condition needed for this to be true?
<ol>
<li> The sample is random. </li>
<li> The sample size, 60, is less than 10% of all houses. </li>
<li> The sample distribution must be nearly normal. </li>
</ol>


## Confidence levels

3.  What does "95% confidence" mean?
<ol>
<li> 95% of the time the true average area of houses in Ames, Iowa, will be in this interval. </li>
<li> 95% of random samples of size 60 will yield confidence intervals that contain the true average area of houses in Ames, Iowa. </li>
<li> 95% of the houses in Ames have an area in this interval. </li>
<li> 95% confident that the sample mean is in this interval. </li>
</ol>

In this case we have the rare luxury of knowing the true population mean since we 
have data on the entire population. Let's calculate this value so that
we can determine if our confidence intervals actually capture it. We'll store it in a
data frame called `params` (short for population parameters), and name it `mu`.

```{r pop-mean}
params <- ames %>%
  summarise(mu = mean(area))
```

<div id="exercise">
**Exercise**: Does your confidence interval capture the true average size of houses in 
Ames?
</div>
```{r check-ci-contain-true-mean}
# type your code for the Exercise here, and Knit
# Yes, the true mean is contained within the confidence interval.
```

4. What proportion of 95% confidence intervals would you expect to capture the true population mean?
<ol>
<li> 1% </li>
<li> 5% </li>
<li> 95% </li>
<li> 99% </li>
</ol>

Using R, we're going to collect many samples to learn more about how sample 
means and confidence intervals vary from one sample to another.

Here is the rough outline:

-   Obtain a random sample.
-   Calculate the sample's mean and standard deviation, and use these to calculate
and store the lower and upper bounds of the confidence intervals.
-   Repeat these steps 50 times.

We can accomplish this using the `rep_sample_n` function. The following lines of 
code takes 50 random samples of size `n` from population (and remember we defined 
$n = 60$ earlier), and computes the upper and lower bounds of the confidence intervals based on these samples.

```{r calculate-50-cis}
set.seed(19122018);
ci <- ames %>%
        rep_sample_n(size = n, reps = 50, replace = TRUE) %>%
        summarise(lower = mean(area) - z_star_95 * (sd(area) / sqrt(n)),
                  upper = mean(area) + z_star_95 * (sd(area) / sqrt(n)), samplemeans.95 = mean(area))
```

Let's view the first five intervals:

```{r first-five-intervals}
#set.seed(19122018);
ci %>%
  slice(1:5)
```


Next we'll create a plot similar to Figure 4.8 on page 175 of [OpenIntro Statistics, 3rd
Edition](https://www.openintro.org/os). First step will be to create a new variable in 
the `ci` data frame that indicates whether the interval does or does not capture the 
true population mean. Note that capturing this value would mean the lower bound of the
confidence interval is below the value and upper bound of the confidence interval is
above the value. Remember that we create new variables using the `mutate` function.

```{r capture-mu}
#set.seed(19122018)
ci <- ci %>%
  mutate(capture_mu = ifelse(lower < params$mu & upper > params$mu, "yes", "no"))
```

The `ifelse` function is new. It takes three arguments: first is a logical statement,
second is the value we want if the logical statement yields a true result, and the
third is the value we want if the logical statement yields a false result.

We now have all the information we need to create the plot, but we need to re-organize
our data a bit for easy plotting. Specifically, we need to organize the data in a new
data frame where each row represents one bound, as opposed to one interval. So this

~~~
     lower    upper capture_mu
1 1350.540 1544.360        yes
2 1333.441 1584.425        yes
3 1412.133 1663.801        yes
...
~~~

should instead look like

~~~
  ci_id ci_bounds capture_mu
1     1  1350.540        yes
2     2  1333.441        yes
3     3  1412.133        yes
4     1  1544.360        yes
5     2  1584.425        yes
6     3  1663.801        yes
...
~~~

We can accomplish this using the following:

```{r create-ci-data-for-plot}
set.seed(19122018)
ci_data <- data.frame(ci_id = c(1:50, 1:50),
                      ci_bounds = c(ci$lower, ci$upper),
                      capture_mu = c(ci$capture_mu, ci$capture_mu))
```

And finally we can create the plot using the following:

```{r plot-ci}
#set.seed(19122018)
ggplot(data = ci_data, aes(x = ci_bounds, y = ci_id, 
                           group = ci_id, color = capture_mu)) +
  geom_point(size = 2) +  # add points at the ends, size = 2
  geom_line() +           # connect with lines
  geom_vline(xintercept = params$mu, color = "darkgray") # draw vertical line
```

## How set.seed() works - testing?
```{r}
set.seed(111112)
ci.95 <- ames %>%
   rep_sample_n(size = n, reps = 50, replace = TRUE) %>%
  summarise(samplemeans.95 = mean(area))
```

```{r}
head(ci.95)
```


```{r}
set.seed(111112)
ci.99 <- ames %>%
  rep_sample_n(size = n, reps = 50, replace = TRUE) %>%
  summarise(samplemeans.99 = mean(area))
```

```{r}
head(ci.99)
```
<div id="exercise">
**Exercise**: What proportion of your confidence intervals include the true population mean? 
Is this proportion exactly equal to the confidence level? If not, explain why.

**Answer**: It is almost all of the CIs = 98% include the true population mean except 1 sample. The proportion is not exactly equal to the confidence level because the confidence interval only provides a plausible range within which the mean is captured. BY definition, we are 95% confident, but results might vary based on how the samples are randonly selected. This is an overestimation of the 95% CI sampling.
</div>


5. What is the appropriate critical value for a 99% confidence level?
<ol>
<li> 0.01 </li>
<li> 0.99 </li> 
<li> 1.96 </li>
<li> 2.33 </li>
<li> 2.58 </li>
</ol>
```{r find-99-perc-crit-val}
# type your code for the Question 5 here, and Knit
set.seed(19122018)
z_star_99 <- qnorm(0.995) 
z_star_99
```

<div id="exercise">
**Exercise**: Calculate 50 confidence intervals at the 99% confidence level. You do not need to obtain new samples, simply calculate new intervals based on the 95% confidence interval endpoints you had already collected. Plot all intervals and calculate the proportion of intervals that include the true population mean.
</div>
```{r plot-99-perc-cis}
# type your code for the Exercise here, and Knit

set.seed(19122018)
#samples for 99% confidence interval
ci_99 <- ames  %>%
        rep_sample_n(size = n ,reps = 50, replace = TRUE) %>%
        summarise(lower = mean(area) - z_star_99 * (sd(area) / sqrt(n)),
                  upper = mean(area) + z_star_99 * (sd(area) / sqrt(n)), samplemeans.99 = mean(area))
#assignment of capture
ci_99 <- ci_99 %>%
  mutate(capture_mu_99 = ifelse(lower < params$mu & upper > params$mu, "yes", "no"))

#datagrame for plot
ci_data_99 <- data.frame(ci_id_99 = c(1:50, 1:50),
                      ci_bounds_99 = c(ci_99$lower, ci_99$upper),
                      capture_mu_99 = c(ci_99$capture_mu_99, ci_99$capture_mu_99))
#plot
ggplot(data = ci_data_99, aes(x = ci_bounds_99, y = ci_id_99, 
                           group = ci_id_99, color = capture_mu_99)) +
  geom_point(size = 2) +  # add points at the ends, size = 2
  geom_line() +           # connect with lines
  geom_vline(xintercept = params$mu, color = "darkblue") # draw vertical line
```

```{r - 99% ci dataframe}

 ci_99 %>%
   slice(1:5)
```

```

6. We would expect 99% of the intervals to contain the true population mean.
<ol>
<li> True </li>
<li> False </li>
</ol>


<div id="license">
This is a product of OpenIntro that is released under a [Creative Commons 
Attribution-ShareAlike 3.0 Unported](http://creativecommons.org/licenses/by-sa/3.0).
This lab was written for OpenIntro by Andrew Bray and Mine &Ccedil;etinkaya-Rundel.
</div>

<!--chapter:end:02_confidence_intervals_Coursera.Rmd-->

# Week 3 {#week3}

Week 3 is about making inferences about numerical data.

---
title: "Inference for numerical data"
output: statsr:::statswithr_lab
---

<div id="instructions">
Complete all **Exercises**, and submit answers to **Questions** on the Coursera 
platform.
</div>

## Getting Started

### Load packages

In this lab we will explore the data using the `dplyr` package and visualize it 
using the `ggplot2` package for data visualization. The data can be found in the
companion package for this course, `statsr`.

Let's load the packages.

```{r load-packages, message=FALSE}
library(statsr)
library(dplyr)
library(ggplot2)
```

### The data

In 2004, the state of North Carolina released a large data set containing 
information on births recorded in this state. This data set is useful to 
researchers studying the relation between habits and practices of expectant 
mothers and the birth of their children. We will work with a random sample of 
observations from this data set.

Load the `nc` data set into our workspace.

```{r load-data}
data(nc)
```

We have observations on 13 different variables, some categorical and some 
numerical. The meaning of each variable is as follows.

variable         | description
---------------- | ---------------------------------------------
`fage`           | father's age in years.
`mage`           | mother's age in years.
`mature`         | maturity status of mother.
`weeks`          | length of pregnancy in weeks.
`premie`         | whether the birth was classified as premature (premie) or full-term.
`visits`         | number of hospital visits during pregnancy.
`marital`        | whether mother is `married` or `not married` at birth.
`gained`         | weight gained by mother during pregnancy in pounds.
`weight`         | weight of the baby at birth in pounds.
`lowbirthweight` | whether baby was classified as low birthweight (`low`) or not (`not low`).
`gender`         | gender of the baby, `female` or `male`.
`habit`          | status of the mother as a `nonsmoker` or a `smoker`.
`whitemom`       | whether mom is `white` or `not white`.

1. There are 1,000 cases in this data set, what do the cases represent? 
<ol>
<li> The hospitals where the births took place </li> 
<li> The fathers of the children </li> 
<li> The days of the births </li>
<li> The births </li>
</ol>

As a first step in the analysis, we should take a look at the variables in the dataset. 
This can be done using the `str` command:

```{r str}
str(nc)
```

As you review the variable summaries, consider which variables are categorical and which 
are numerical. For numerical variables, are there outliers? If you aren't sure or want to 
take a closer look at the data, make a graph.

## Exploratory data analysis

We will first start with analyzing the weight gained by mothers throughout the 
pregnancy: `gained`.

Using visualization and summary statistics, describe the distribution of weight 
gained by mothers during pregnancy. The `summary` function can also be useful.

```{r summary}
summary(nc$gained)
```

2. How many mothers are we missing weight gain data from?
<ol>
<li> 0 </li>
<li> 13 </li>
<li> 27 </li>
<li> 31 </li>
</ol>

Next, consider the possible relationship between a mother's smoking habit and the 
weight of her baby. Plotting the data is a useful first step because it helps 
us quickly visualize trends, identify strong associations, and develop research
questions.

3. Make side-by-side boxplots of `habit` and `weight`. Which of the following is 
false about the relationship between habit and weight?
<ol>
<li> Median birth weight of babies born to non-smoker mothers is slightly higher than that of babies born to smoker mothers. </li>
<li> Range of birth weights of babies born to non-smoker mothers is greater than that of babies born to smoker mothers. </li>
<li> Both distributions are extremely right skewed. </li>
<li> The IQRs of the distributions are roughly equal. </li>
</ol>
```{r habit-weight-box}
# type your code for the Question 3 here, and Knit

#boxplot(nc$habit,nc$weight)
ggplot(nc, aes(x=habit, y=weight)) + geom_boxplot()
```

The box plots show how the medians of the two distributions compare, but we can
also compare the means of the distributions using the following to 
first group the data by the `habit` variable, and then calculate the mean
`weight` in these groups using the `mean` function.

```{r by-means}
nc %>%
  group_by(habit) %>%
  summarise(mean_weight = mean(weight))
```

There is an observed difference, but is this difference statistically 
significant? In order to answer this question we will conduct a hypothesis 
test.

## Inference


<div id="exercise">
**Exercise**: Are all conditions necessary for inference satisfied? Comment on each. You can compute the group sizes using the same `by` command above but replacing `mean(weight)` with `n()`.
</div>

<div id = "answer">
**Answer**: This is the case of inference where two means are compared and t-distribution will be used. The categories are independent by definition and the number of samples are assumed to be  <10% of the entire population as there would be many number of cases. Both the group distributions are not very skewed as per the box plots above.
</div>

```{r}
nc %>%
  group_by(habit) %>%
  summarise(no_of_cases = n())
```


4.  What are the hypotheses for testing if the average weights of babies born to 
smoking and non-smoking mothers are different?
<ol>
<li> $H_0: \mu_{smoking} = \mu_{non-smoking}$; $H_A: \mu_{smoking} > \mu_{non-smoking}$ </li>
<li> $H_0: \mu_{smoking} = \mu_{non-smoking}$; $H_A: \mu_{smoking} \ne \mu_{non-smoking}$ </li>
<li> $H_0: \bar{x}_{smoking} = \bar{x}_{non-smoking}$; $H_A: \bar{x}_{smoking} > \bar{x}_{non-smoking}$ </li>
<li> $H_0: \bar{x}_{smoking} = \bar{x}_{non-smoking}$; $H_A: \bar{x}_{smoking} > \bar{x}_{non-smoking}$ </li>
<li> $H_0: \mu_{smoking} \ne \mu_{non-smoking}$;  $H_A: \mu_{smoking} = \mu_{non-smoking}$ </li>
</ol>

Next, we introduce a new function, `inference`, that we will use for conducting
hypothesis tests and constructing confidence intervals. 

Then, run the following:

```{r inf-weight-habit-ht, tidy=FALSE}
inference(y = weight, x = habit, data = nc, statistic = "mean", type = "ht", null = 0, 
          alternative = "twosided", method = "theoretical")
```

Let's pause for a moment to go through the arguments of this custom function. 
The first argument is `y`, which is the response variable that we are 
interested in: `weight`. The second argument is the explanatory variable, 
`x`, which is the variable that splits the data into two groups, smokers and 
non-smokers: `habit`. The third argument, `data`, is the data frame these
variables are stored in. Next is `statistic`, which is the sample statistic
we're using, or similarly, the population parameter we're estimating. In future labs
we can also work with "median" and "proportion". Next we decide on the `type` of inference 
we want: a hypothesis test (`"ht"`) or a confidence interval (`"ci"`). When performing a 
hypothesis test, we also need to supply the `null` value, which in this case is `0`, 
since the null hypothesis sets the two population means equal to each other. 
The `alternative` hypothesis can be `"less"`, `"greater"`, or `"twosided"`. 
Lastly, the `method` of inference can be `"theoretical"` or `"simulation"` based.

For more information on the inference function see the help file with `?inference`.

<div id="exercise">
**Exercise**: What is the conclusion of the hypothesis test?
</div>

5.  Change the `type` argument to `"ci"` to construct and record a confidence 
interval for the difference between the weights of babies born to nonsmoking and 
smoking mothers, and interpret this interval in context of the data. Note that by 
default you'll get a 95% confidence interval. If you want to change the
confidence level, add a new argument (`conf_level`) which takes on a value
between 0 and 1. Also note that when doing a confidence interval arguments like
`null` and `alternative` are not useful, so make sure to remove them.
<ol>
<li> We are 95% confident that babies born to nonsmoker mothers are on average 0.05 to 0.58 pounds lighter at birth than babies born to smoker mothers. </li>
<li> We are 95% confident that the difference in average weights of babies whose moms are smokers and nonsmokers is between 0.05 to 0.58 pounds. </li>
<li> We are 95% confident that the difference in average weights of babies in this sample whose moms are smokers and nonsmokers is between 0.05 to 0.58 pounds. </li>
<li> We are 95% confident that babies born to nonsmoker mothers are on average 0.05 to 0.58 pounds heavier at birth than babies born to smoker mothers. </li>
</ol>
```{r habit-weight-ci}
# type your code for the Question 5 here, and Knit
inference(y = weight, x = habit, data = nc, statistic = "mean", type = "ci", method = "theoretical")
```

By default the function reports an interval for ($\mu_{nonsmoker} - \mu_{smoker}$)
. We can easily change this order by using the `order` argument:

```{r inf-weight-habit-ci, tidy=FALSE}
inference(y = weight, x = habit, data = nc, statistic = "mean", type = "ci", 
          method = "theoretical", order = c("smoker","nonsmoker"))
```

6. Calculate a 99% confidence interval for the average length of pregnancies 
(`weeks`). Note that since you're doing inference on a single population 
parameter, there is no explanatory variable, so you can omit the `x` variable 
from the function. Which of the following is the correct interpretation of this 
interval?
<ol>
<li>(38.1526 , 38.5168)</li>
<li>(38.0892 , 38.5661)</li>
<li>(6.9779 , 7.2241)</li>
<li>(38.0952 , 38.5742)</li>
</ol>
```{r weeks-ci-99}
# type your code for Question 6 here, and Knit
inference(y = weeks, data = nc, statistic = "mean", type = "ci", conf_level = 0.99, method = "theoretical")

```

<div id="exercise">
**Exercise**: Calculate a new confidence interval for the same parameter at the 90% confidence level. Comment on the width of this interval versus the one obtained in the the previous exercise.
</div>
**Answer**: This CI is narrower compared to the 99% CI.
```{r weeks-ci-90}
# type your code for the Exercise here, and Knit

inference(y = weeks, data = nc, statistic = "mean", type = "ci", conf_level = 0.90, method = "theoretical")
```

<div id="exercise">
**Exercise**: Conduct a hypothesis test evaluating whether the average weight gained by younger mothers is different than the average weight gained by mature mothers.
</div>
```{r gained-mature-ht}
# type your code for the Exercise here, and Knit
#str(nc)
inference(y = weight, x = mature, data = nc, statistic = "mean", type = "ht", method = "theoretical", alternative = "twosided", null = 0)
```

7. Now, a non-inference task: Determine the age cutoff for younger and mature 
mothers. Use a method of your choice, and explain how your method works.
```{r cutoff-mature}
# type your code for Question 7 here, and Knit
#str(nc)
nc %>%
  group_by(mature) %>% #blocking for maturity by this line of code
  summarise(min = min(mage), max = max(mage)) # summarizing the ages in a table
```

<div id="exercise">
**Exercise**: Pick a pair of variables: one numerical (response) and one categorical 
(explanatory). Come up with a research question evaluating the relationship between 
these variables. Formulate the question in a way that it can be answered using a 
hypothesis test and/or a confidence interval. Answer your question using the `inference` 
function, report the statistical results, and also provide an explanation in 
plain language. Be sure to check all assumptions,state your $\alpha$ level, and conclude 
in context. (Note: Picking your own variables, coming up with a research question,
and analyzing the data to answer this question is basically what you'll need to do for
your project as well.)
</div>

```{r pick-your-own}
# type your code for the Exercise here, and Knit

```


<div id="license">
This is a product of OpenIntro that is released under a [Creative Commons 
Attribution-ShareAlike 3.0 Unported](http://creativecommons.org/licenses/by-sa/3.0).
This lab was written for OpenIntro by Andrew Bray and Mine &Ccedil;etinkaya-Rundel.
</div>

<!--chapter:end:03_inf_for_numerical_data_Coursera.Rmd-->

# Week 4 {#week4}

Week 4 is about making inference from categorical data.



---
title: "Inference for categorical data"
output: statsr:::statswithr_lab
---

<div id="instructions">
Complete all **Exercises**, and submit answers to **Questions** on the Coursera 
platform.
</div>

In August of 2012, news outlets ranging from the [Washington Post](http://www.washingtonpost.com/national/on-faith/poll-shows-atheism-on-the-rise-in-the-us/2012/08/13/90020fd6-e57d-11e1-9739-eef99c5fb285_story.html) to the [Huffington Post](http://www.huffingtonpost.com/2012/08/14/atheism-rise-religiosity-decline-in-america_n_1777031.html) ran a story about the rise of atheism in America. The source for the story was a poll that asked people, "Irrespective of whether you attend a place of worship or not, would you say you are a religious person, not a religious person or a convinced atheist?" This type of question, which asks people to classify themselves in one way or another, is common in polling and generates categorical data. In this lab we take a look at the atheism survey and explore what's at play when making inference about population proportions using categorical data.

## Getting Started

### Load packages

In this lab we will explore the data using the `dplyr` package and visualize it 
using the `ggplot2` package for data visualization. The data can be found in the
companion package for this course, `statsr`.

Let's load the packages.

```{r load-packages, message=FALSE}
library(statsr)
library(dplyr)
library(ggplot2)
```

### The survey

The press release for the poll, conducted by WIN-Gallup International, can be accessed [here](https://www.scribd.com/document/136318147/Win-gallup-International-Global-Index-of-Religiosity-and-Atheism-2012).

Take a moment to review the report then address the following questions.

1. How many people were interviewed for this survey? 
<ol>
<li> A poll conducted by WIN-Gallup International surveyed 51,000 people from 57 countries. </li>
<li> A poll conducted by WIN-Gallup International surveyed 52,000 people from 57 countries. </li>
<li> A poll conducted by WIN-Gallup International surveyed 51,917 people from 57 countries. </li>
<li> A poll conducted by WIN-Gallup International surveyed 51,927 people from 57 countries. </li>
</ol>

2. Which of the following methods were used to gather information? 
<ol>
<li> Face to face </li> 
<li> Telephone </li>
<li> Internet </li> 
<li> All of the above </li>
</ol>

3. True / False: In the first paragraph, several key findings are reported. These percentages appear to be **sample statistics**. 
<ol>
<li> True </li> 
<li> False </li>
</ol>

4. True / False:The title of the report is "Global Index of Religiosity and Atheism". To generalize the report's findings to the global human population, We must assume that the sample was a random sample from the entire population in order to be able to generalize the results to the global human population. This seems to be a reasonable assumption.
<ol>
<li> True </li> 
<li> False </li>
</ol>

### The data

Turn your attention to Table 6 (pages 15 and 16), which reports the sample size and response percentages for all 57 countries. While this is a useful format to summarize the data, we will base our analysis on the original data set of individual responses to the survey. Load this data set into R with the following command.

```{r load-data}
data(atheism)
```

5. What does each row of Table 6 correspond to? 
<ol>
<li> Countries </li> 
<li> Individual Persons </li> 
<li> Religions </li>
</ol>

6. What does each row of `atheism` correspond to?
<ol>
<li> Countries </li> 
<li> Individual Persons </li> 
<li> Religions </li>
</ol>

To investigate the link between these two ways of organizing this data, take a look at the estimated proportion of atheists in the United States. Towards the bottom of Table 6, we see that this is 5%. We should be able to come to the same number using the `atheism` data.

Create a new dataframe called `us12` that contains only the rows in `atheism` associated with respondents to the 2012 survey from the United States:

```{r us-atheism}
us12 <- atheism %>%
  filter(nationality == "United States" , atheism$year == "2012")
```


7. Next, calculate the proportion of atheist responses in the United States in 2012, i.e. in `us12`. True / False: This percentage agrees with the percentage in Table~6.
<ol>
<li> True </li> 
<li> False </li>
</ol>
```{r perc-atheist-us12}
# type your code for Question 7 here, and Knit
us12 %>%
  group_by(response) %>%
  summarise(n = n()) %>%
  # Show percentage of each level within the categorical variable
  mutate(perc = n/sum(n)) 


```


## Inference on proportions

As was hinted earlier, Table 6 provides **sample statistics**, that is, calculations made from the sample of 51,927 people. What we'd like, though, is insight into the population **population parameters**. You answer the question, "What proportion of people in your sample reported being atheists?" with a statistic; while the question "What proportion of people on earth would report being atheists" is answered with an estimate of the parameter.

The inferential tools for estimating population proportion are analogous to those used for means in the last lab: the confidence interval and the hypothesis test.

<div id="exercise">
**Exercise**: Write out the conditions for inference to construct a 95% confidence interval for the proportion of atheists in the United States in 2012. Are you confident all conditions are met?
</div>

If the conditions for inference are reasonable, we can either calculate the standard error and construct the interval by hand, or allow the `inference` function to do it for us.

```{r us-atheism-ci}
inference(y = response, data = us12, statistic = "proportion", type = "ci", method = "theoretical", success = "atheist")
```

Note that since the goal is to construct an interval estimate for a proportion, it's necessary to specify what constitutes a ``success'', which here is a response of `atheist`.

Although formal confidence intervals and hypothesis tests don't show up in the report, suggestions of inference appear at the bottom of page 7: "In general, the error margin for surveys of this kind is $\pm$ 3-5% at 95% confidence."

<div id="exercise">
**Exercise**: Imagine that, after reading a front page story about the latest public opinion poll, a family member asks you, "What is a margin of error?" In one sentence, and ignoring the mechanics behind the calculation, how would you respond in a way that conveys the general concept?
</div>
<div id="answer">
**Answer**: To represent the entire population there will be an offset from the point estimate either on the positive or the negative side based on the confidence level we need.
</div>

8. Based on the R output, what is the margin of error for the estimate of the proportion of the proportion of atheists in US in 2012? 
<ol>
<li> The margin of error for the estimate of the proportion of atheists in the US in 2012 is 0.05. </li> 
<li> The margin of error for the estimate of the proportion of atheists in the US in 2012 is 0.025. </li> 
<li> The margin of error for the estimate of the proportion of atheists in the US in 2012 is 0.0135. </li>
</ol>
```{r me-perc-atheist-us12}
# type your code for Question 8 here, and Knit
n <- 1002
p_hat <- 0.0499
se <- sqrt(p_hat*(1-(p_hat))/n)
moe <- qnorm(0.975)*se
moe

           
```

<div id="exercise">
**Exercise**: Using the inference function, calculate confidence intervals for the proportion of atheists in 2012 in two other countries of your choice, and report the associated margins of error. Be sure to note whether the conditions for inference are met. It may be helpful to create new data sets for each of the two countries first, and then use these data sets in the `inference` function to construct the confidence intervals.
</div>
```{r me-perc-atheist-other-countries}
# type your code for the Exercise here, and Knit

```

## How does the proportion affect the margin of error?

Imagine you've set out to survey 1000 people on two questions: are you female? and are you left-handed? Since both of these sample proportions were calculated from the same sample size, they should have the same margin of error, right? Wrong!  While the margin of error does change with sample size, it is also affected by the proportion.

Think back to the formula for the standard error: $SE = \sqrt{p(1-p)/n}$. This is then used in the formula for the margin of error for a 95% confidence interval: $ME = 1.96\times SE = 1.96\times\sqrt{p(1-p)/n}$. Since the population proportion $p$ is in this $ME$ formula, it should make sense that the margin of error is in some way dependent on the population proportion. We can visualize this relationship by creating a plot of $ME$ vs. $p$.

The first step is to make a vector `p` that is a sequence from $0$ to $1$ with each number separated by $0.01$. We can then create a vector of the margin of error (`me`) associated with each of these values of `p` using the familiar approximate formula ($ME = 1.96 \times SE$). Lastly, we plot the two vectors against each other to reveal their relationship.

```{r me-plot}
d <- data.frame(p <- seq(0, 1, 0.01))
n <- 1000
d <- d %>%
  mutate(me = 1.96*sqrt(p*(1 - p)/n))
ggplot(d, aes(x = p, y = me)) +
  geom_line()
```

9. Which of the following is false about the relationship between $p$ and $ME$. 
<ol>
<li> The $ME$ reaches a minimum at $p = 0$. </li> 
<li> The $ME$ reaches a minimum at $p = 1$. </li> 
<li> The $ME$ is maximized when $p = 0.5$. </li> 
<li> The most conservative estimate when calculating a confidence interval occurs when $p$ is set to 1. </li>
</ol>

The question of atheism was asked by WIN-Gallup International in a similar survey that was conducted in 2005. We assume here that sample sizes have remained the same. Table 4 on page 13 of the report summarizes survey results from 2005 and 2012 for 39 countries.

Answer the following two questions using the `inference` function. As always, write out the hypotheses for any tests you conduct and outline the status of the conditions for inference.

10. True / False: There is convincing evidence that Spain has seen a change in its atheism index between 2005 and 2012. <br><br> *Hint:* Create a new data set for respondents from Spain. Then use their responses as the first input on the `inference`, and use `year` as the grouping variable.
<ol>
<li> True </li> 
<li> False </li>
</ol>
```{r spain-05-12}
# type your code for Question 10 here, and Knit
spain <- atheism %>%
  filter(nationality == "Spain") %>%
  group_by(year)

inference(y = response, x = as.factor(year), data = spain, statistic = "proportion", method = "theoretical", type = "ht", success = "atheist", alternative = "twosided", null = 0 )
```

11. True / False: There is convincing evidence that the United States has seen a change in its atheism index between 2005 and 2012.
<ol>
<li> True </li> 
<li> False </li>
</ol>
```{r us-05-12}
# type your code for Question 11 here, and Knit
us <- atheism %>%
  filter(nationality == "United States")

inference(y = response, data = us, x = as.factor(year), method = "theoretical", type = "ht", alternative = "twosided", null = 0, statistic = "proportion", success = "atheist")
```


12. If in fact there has been no change in the atheism index in the countries listed in Table 4, in how many of those countries would you expect to detect a change (at a significance level of 0.05) simply by chance? <br><br> *Hint:* Type 1 error.
<ol>
<li> 0 </li>
<li> 1 </li> 
<li> 1.95 </li> 
<li> 5 </li>
</ol>
```{r type1}
# type your code for Question 12 here, and Knit
sig_level = 0.05
nt4 = 39
Type_1_error <- sig_level * nt4

Type_1_error
```

13. Suppose you're hired by the local government to estimate the proportion of residents that attend a religious service on a weekly basis. According to the guidelines, the estimate must have a margin of error no greater than 1% with 95% confidence. You have no idea what to expect for $p$. How many people would you have to sample to ensure that you are within the guidelines? <br><br> *Hint:* Refer to your plot of the relationship between $p$ and margin of error. Do not use the data set to answer this question.
<ol>
<li> 2401 people </li>
<li> At least 2401 people </li>
<li> 9604 people </li> 
<li> At least 9604 people </li>
</ol>
```{r sample-size}
# type your code for Question 13 here, and Knit
p <- 0.5
z_crit <- qnorm(0.975)
n_sample <- z_crit^2 * p*(1-p)/0.01^2
n_sample

```

<div id="license">
This is a product of OpenIntro that is released under a [Creative Commons 
Attribution-ShareAlike 3.0 Unported](http://creativecommons.org/licenses/by-sa/3.0).
This lab was written for OpenIntro by Andrew Bray and Mine &Ccedil;etinkaya-Rundel.
</div>

<!--chapter:end:04_inf_for_categorical_data_Coursera.Rmd-->

# Week 5 {#week5}

Week 5 entails a mini-project which covers framing the research question, doing exploratory data analysis and answering the research question with the help of statistical inference.

---
title: "Statistical inference with the GSS data - Akshay Kotha"
---

## Setup

### Load packages

```{r load-packages, message = FALSE}
library(ggplot2)
library(dplyr)
library(statsr)
library(vcd)
```

### Load data

```{r load-data}
load("gss.Rdata")
```


* * *

## Part 1: Data

**Describe how the observations in the sample are collected, and the implications of this data collection method on the scope of inference (generalizability / causality)?**

The data was collected by full probability sampling commonly called random sampling across years by helding personal interviews. Within this, there were stratified samplings by different parameters like age, region, race, income. This is an interview survey so it is voluntary and nothing is controlled which makes it an observational study. This can be generalized to the United States population and statistical significance can be found to know the extent to which the samples' data is representative of the entire US population. Causal inference can't be derived as there is no random assignment rather there are observations of independently and randomly made samples. There is also non-response subsampling which would help in reducing the non-response bias. Reducing this bias assures the statistical significance and makes the chosen samples more representative of the entire US population.


* * *

## Part 2: Research question

**Is there an association between race of the respondent and natrace (an attitudinal measure about Improving the conditions of blacks)?**

*natrace has 3 levels: Too Little, About Right, Too Much*

*This is of interest because*:

The association if it exists helps us understand to further engineer the variables and create new variables for predictive models. More importantly, the statistical inference within the sample represents the entire population which otherwise would not be known. This is also for a quantitative find about how the race of a person makes the reply to this question of interest in the variable `natrace`.

* * *

## Part 3: Exploratory data analysis

```{r}
#to check the variable names
#str(gss)
```

```{r}
#filtering out null values
conting_sb <- gss %>%
  filter(!is.na(natrace), !is.na(race))

#segmented bar plot
ggplot(conting_sb, aes(x = race)) + geom_bar(aes(fill = natrace), position = 'fill') + labs(x = "race", y = "proportion of natrace response")


```

**Narrative**: 

The proportion of blacks who has the opinion that 'Too little' is spent on improving the conditions of blacks is much higher compared to 'Whites' and Other races. This matched my intuition. To understand if this is statistically significant, we will find out the association between these variables using chi-square test of independence. There is mosaic plot too coming up which best represents the contingency table.

* * *

## Part 4: Inference

**Hypotheses**:

H0: There is no association between the categorical variables `race` and `natrace`.
HA: There is a relationship between the categorical variables `race` and `natrace`.


```{r}
#creating a contingency table
ct <- table(gss$race, gss$natrace)
ct
```
**Conditions**:

***Independence***: 

1. Random sampling of the entire sample was mentioned in the GSS study.
2. When it comes to each group, by seeing the contingency table above, the number of cases are definitely less than 10% of respective populations. The respective populations would be way higher.
3. Each case definitely contributed to one specific case and to remove any ambiguity the `NA` values have been filtered out.

***Sample size***:

Each particular cell has greater than or equal to 5 cases as per the contingency table.
```{r}
cont <- as.table(as.matrix(ct))
mosaicplot(cont, shade = TRUE, las=2, main = "Mosaic Plot between race and natrace")
```

*Higher standardized residuals means higher frequency of cases indicated by blue color boxes. Majority of whites are categorized under 'About Right' or 'Too Much' category while majortiy of blacks are categorized under 'Too Little'*

**Method to be used and why and how**:

We will be using chi-square test of independence to find out the relationship between these two categorical variables. Both the variables have more than 2 categorical levels and the conditions for the test of independence (TOI) are satisfied.

```{r}
# Calculating chi-square statistic and the corresponding p-value
chisq_race_natrace <- chisq.test(ct)
chisq_race_natrace
```


```{r}
# expected counts in each cell of the contingency table
round(chisq_race_natrace$expected,2)
```
**Results**:

Based on the high value of chi-square statistic and the degrees of freedom under consideration, we have a p-value which is very tiny compared to the 5% significance level. This implies there is a clear association/ relationship between the variables `race` and `natrace`. Further analysis can be carried out to find out if it is a positive correlation or a negative correlation. Hoping to learn that soon. The confidence interval calcualtion is not included as it not associated with chi-square testing and that is because chi-square distribution is always right skewed. New techniques have to be implemented to make the distribution normal and then CLT or confidence interval can be applied.

Created by: [Akshay Kotha](https://twitter.com/akshaykreddy)

<!--chapter:end:05_stat_inf_project_inferential_statistics.Rmd-->

